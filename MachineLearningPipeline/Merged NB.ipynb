{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87287e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading K2 planets and candidates dataset...\n",
      "K2 dataset shape: (4004, 94)\n",
      "\n",
      "Loading Kepler cumulative dataset...\n",
      "Kepler dataset shape: (9564, 141)\n",
      "\n",
      "==================================================\n",
      "DATASET OVERVIEW\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# File paths for the two datasets\n",
    "k2_file = r\"C:\\Users\\mobed\\Desktop\\Nasa Space Apps\\ML\\raw\\k2pandc_2025.10.01_03.31.10.csv\"\n",
    "kepler_file = r\"C:\\Users\\mobed\\Desktop\\Nasa Space Apps\\ML\\raw\\cumulative_2025.10.02_13.38.11.csv\"\n",
    "\n",
    "# Load the datasets\n",
    "print(\"Loading K2 planets and candidates dataset...\")\n",
    "k2_df = pd.read_csv(k2_file)\n",
    "print(f\"K2 dataset shape: {k2_df.shape}\")\n",
    "\n",
    "print(\"\\nLoading Kepler cumulative dataset...\")\n",
    "kepler_df = pd.read_csv(kepler_file)\n",
    "print(f\"Kepler dataset shape: {kepler_df.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"DATASET OVERVIEW\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8cf232ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K2 DATASET COLUMNS:\n",
      "------------------------------\n",
      " 1. pl_name\n",
      " 2. hostname\n",
      " 3. default_flag\n",
      " 4. disposition\n",
      " 5. disp_refname\n",
      " 6. sy_snum\n",
      " 7. sy_pnum\n",
      " 8. discoverymethod\n",
      " 9. disc_year\n",
      "10. disc_facility\n",
      "11. soltype\n",
      "12. pl_controv_flag\n",
      "13. pl_refname\n",
      "14. pl_orbper\n",
      "15. pl_orbpererr1\n",
      "16. pl_orbpererr2\n",
      "17. pl_orbperlim\n",
      "18. pl_orbsmax\n",
      "19. pl_orbsmaxerr1\n",
      "20. pl_orbsmaxerr2\n",
      "21. pl_orbsmaxlim\n",
      "22. pl_rade\n",
      "23. pl_radeerr1\n",
      "24. pl_radeerr2\n",
      "25. pl_radelim\n",
      "26. pl_radj\n",
      "27. pl_radjerr1\n",
      "28. pl_radjerr2\n",
      "29. pl_radjlim\n",
      "30. pl_bmasse\n",
      "31. pl_bmasseerr1\n",
      "32. pl_bmasseerr2\n",
      "33. pl_bmasselim\n",
      "34. pl_bmassj\n",
      "35. pl_bmassjerr1\n",
      "36. pl_bmassjerr2\n",
      "37. pl_bmassjlim\n",
      "38. pl_bmassprov\n",
      "39. pl_orbeccen\n",
      "40. pl_orbeccenerr1\n",
      "41. pl_orbeccenerr2\n",
      "42. pl_orbeccenlim\n",
      "43. pl_insol\n",
      "44. pl_insolerr1\n",
      "45. pl_insolerr2\n",
      "46. pl_insollim\n",
      "47. pl_eqt\n",
      "48. pl_eqterr1\n",
      "49. pl_eqterr2\n",
      "50. pl_eqtlim\n",
      "51. ttv_flag\n",
      "52. st_refname\n",
      "53. st_spectype\n",
      "54. st_teff\n",
      "55. st_tefferr1\n",
      "56. st_tefferr2\n",
      "57. st_tefflim\n",
      "58. st_rad\n",
      "59. st_raderr1\n",
      "60. st_raderr2\n",
      "61. st_radlim\n",
      "62. st_mass\n",
      "63. st_masserr1\n",
      "64. st_masserr2\n",
      "65. st_masslim\n",
      "66. st_met\n",
      "67. st_meterr1\n",
      "68. st_meterr2\n",
      "69. st_metlim\n",
      "70. st_metratio\n",
      "71. st_logg\n",
      "72. st_loggerr1\n",
      "73. st_loggerr2\n",
      "74. st_logglim\n",
      "75. sy_refname\n",
      "76. rastr\n",
      "77. ra\n",
      "78. decstr\n",
      "79. dec\n",
      "80. sy_dist\n",
      "81. sy_disterr1\n",
      "82. sy_disterr2\n",
      "83. sy_vmag\n",
      "84. sy_vmagerr1\n",
      "85. sy_vmagerr2\n",
      "86. sy_kmag\n",
      "87. sy_kmagerr1\n",
      "88. sy_kmagerr2\n",
      "89. sy_gaiamag\n",
      "90. sy_gaiamagerr1\n",
      "91. sy_gaiamagerr2\n",
      "92. rowupdate\n",
      "93. pl_pubdate\n",
      "94. releasedate\n",
      "\n",
      "Total K2 columns: 94\n",
      "\n",
      "==================================================\n",
      "KEPLER DATASET COLUMNS:\n",
      "------------------------------\n",
      " 1. rowid\n",
      " 2. kepid\n",
      " 3. kepoi_name\n",
      " 4. kepler_name\n",
      " 5. koi_disposition\n",
      " 6. koi_vet_stat\n",
      " 7. koi_vet_date\n",
      " 8. koi_pdisposition\n",
      " 9. koi_score\n",
      "10. koi_fpflag_nt\n",
      "11. koi_fpflag_ss\n",
      "12. koi_fpflag_co\n",
      "13. koi_fpflag_ec\n",
      "14. koi_disp_prov\n",
      "15. koi_comment\n",
      "16. koi_period\n",
      "17. koi_period_err1\n",
      "18. koi_period_err2\n",
      "19. koi_time0bk\n",
      "20. koi_time0bk_err1\n",
      "21. koi_time0bk_err2\n",
      "22. koi_time0\n",
      "23. koi_time0_err1\n",
      "24. koi_time0_err2\n",
      "25. koi_eccen\n",
      "26. koi_eccen_err1\n",
      "27. koi_eccen_err2\n",
      "28. koi_longp\n",
      "29. koi_longp_err1\n",
      "30. koi_longp_err2\n",
      "31. koi_impact\n",
      "32. koi_impact_err1\n",
      "33. koi_impact_err2\n",
      "34. koi_duration\n",
      "35. koi_duration_err1\n",
      "36. koi_duration_err2\n",
      "37. koi_ingress\n",
      "38. koi_ingress_err1\n",
      "39. koi_ingress_err2\n",
      "40. koi_depth\n",
      "41. koi_depth_err1\n",
      "42. koi_depth_err2\n",
      "43. koi_ror\n",
      "44. koi_ror_err1\n",
      "45. koi_ror_err2\n",
      "46. koi_srho\n",
      "47. koi_srho_err1\n",
      "48. koi_srho_err2\n",
      "49. koi_fittype\n",
      "50. koi_prad\n",
      "51. koi_prad_err1\n",
      "52. koi_prad_err2\n",
      "53. koi_sma\n",
      "54. koi_sma_err1\n",
      "55. koi_sma_err2\n",
      "56. koi_incl\n",
      "57. koi_incl_err1\n",
      "58. koi_incl_err2\n",
      "59. koi_teq\n",
      "60. koi_teq_err1\n",
      "61. koi_teq_err2\n",
      "62. koi_insol\n",
      "63. koi_insol_err1\n",
      "64. koi_insol_err2\n",
      "65. koi_dor\n",
      "66. koi_dor_err1\n",
      "67. koi_dor_err2\n",
      "68. koi_limbdark_mod\n",
      "69. koi_ldm_coeff4\n",
      "70. koi_ldm_coeff3\n",
      "71. koi_ldm_coeff2\n",
      "72. koi_ldm_coeff1\n",
      "73. koi_parm_prov\n",
      "74. koi_max_sngle_ev\n",
      "75. koi_max_mult_ev\n",
      "76. koi_model_snr\n",
      "77. koi_count\n",
      "78. koi_num_transits\n",
      "79. koi_tce_plnt_num\n",
      "80. koi_tce_delivname\n",
      "81. koi_quarters\n",
      "82. koi_bin_oedp_sig\n",
      "83. koi_trans_mod\n",
      "84. koi_model_dof\n",
      "85. koi_model_chisq\n",
      "86. koi_datalink_dvr\n",
      "87. koi_datalink_dvs\n",
      "88. koi_steff\n",
      "89. koi_steff_err1\n",
      "90. koi_steff_err2\n",
      "91. koi_slogg\n",
      "92. koi_slogg_err1\n",
      "93. koi_slogg_err2\n",
      "94. koi_smet\n",
      "95. koi_smet_err1\n",
      "96. koi_smet_err2\n",
      "97. koi_srad\n",
      "98. koi_srad_err1\n",
      "99. koi_srad_err2\n",
      "100. koi_smass\n",
      "101. koi_smass_err1\n",
      "102. koi_smass_err2\n",
      "103. koi_sage\n",
      "104. koi_sage_err1\n",
      "105. koi_sage_err2\n",
      "106. koi_sparprov\n",
      "107. ra\n",
      "108. dec\n",
      "109. koi_kepmag\n",
      "110. koi_gmag\n",
      "111. koi_rmag\n",
      "112. koi_imag\n",
      "113. koi_zmag\n",
      "114. koi_jmag\n",
      "115. koi_hmag\n",
      "116. koi_kmag\n",
      "117. koi_fwm_stat_sig\n",
      "118. koi_fwm_sra\n",
      "119. koi_fwm_sra_err\n",
      "120. koi_fwm_sdec\n",
      "121. koi_fwm_sdec_err\n",
      "122. koi_fwm_srao\n",
      "123. koi_fwm_srao_err\n",
      "124. koi_fwm_sdeco\n",
      "125. koi_fwm_sdeco_err\n",
      "126. koi_fwm_prao\n",
      "127. koi_fwm_prao_err\n",
      "128. koi_fwm_pdeco\n",
      "129. koi_fwm_pdeco_err\n",
      "130. koi_dicco_mra\n",
      "131. koi_dicco_mra_err\n",
      "132. koi_dicco_mdec\n",
      "133. koi_dicco_mdec_err\n",
      "134. koi_dicco_msky\n",
      "135. koi_dicco_msky_err\n",
      "136. koi_dikco_mra\n",
      "137. koi_dikco_mra_err\n",
      "138. koi_dikco_mdec\n",
      "139. koi_dikco_mdec_err\n",
      "140. koi_dikco_msky\n",
      "141. koi_dikco_msky_err\n",
      "\n",
      "Total Kepler columns: 141\n",
      "\n",
      "==================================================\n",
      "COMMON COLUMNS:\n",
      "------------------------------\n",
      "Found 2 common columns:\n",
      "  - dec\n",
      "  - ra\n"
     ]
    }
   ],
   "source": [
    "# Examine columns in both datasets\n",
    "print(\"K2 DATASET COLUMNS:\")\n",
    "print(\"-\" * 30)\n",
    "k2_columns = list(k2_df.columns)\n",
    "for i, col in enumerate(k2_columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal K2 columns: {len(k2_columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"KEPLER DATASET COLUMNS:\")\n",
    "print(\"-\" * 30)\n",
    "kepler_columns = list(kepler_df.columns)\n",
    "for i, col in enumerate(kepler_columns):\n",
    "    print(f\"{i+1:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nTotal Kepler columns: {len(kepler_columns)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"COMMON COLUMNS:\")\n",
    "print(\"-\" * 30)\n",
    "common_columns = set(k2_columns).intersection(set(kepler_columns))\n",
    "print(f\"Found {len(common_columns)} common columns:\")\n",
    "for col in sorted(common_columns):\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5c55306c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K2 DATASET INFO:\n",
      "--------------------\n",
      "Shape: (4004, 94)\n",
      "Columns: 94\n",
      "Sample columns: ['pl_name', 'hostname', 'default_flag', 'disposition', 'disp_refname', 'sy_snum', 'sy_pnum', 'discoverymethod', 'disc_year', 'disc_facility']\n",
      "\n",
      "KEPLER DATASET INFO:\n",
      "--------------------\n",
      "Shape: (9564, 141)\n",
      "Columns: 141\n",
      "Sample columns: ['rowid', 'kepid', 'kepoi_name', 'kepler_name', 'koi_disposition', 'koi_vet_stat', 'koi_vet_date', 'koi_pdisposition', 'koi_score', 'koi_fpflag_nt']\n",
      "'pl_name' found in K2 only\n",
      "'hostname' found in K2 only\n",
      "'kepid' found in Kepler only\n",
      "'kepoi_name' found in Kepler only\n",
      "\n",
      "Common identifier columns: []\n",
      "\n",
      "K2 first few columns preview:\n",
      "             pl_name        hostname  default_flag disposition  \\\n",
      "0  EPIC 201111557.01  EPIC 201111557             1   CANDIDATE   \n",
      "1  EPIC 201111557.01  EPIC 201111557             0   CANDIDATE   \n",
      "2  EPIC 201126503.01  EPIC 201126503             1   CANDIDATE   \n",
      "\n",
      "             disp_refname  \n",
      "0  Livingston et al. 2018  \n",
      "1  Livingston et al. 2018  \n",
      "2  Vanderburg et al. 2016  \n",
      "\n",
      "Kepler first few columns preview:\n",
      "   rowid     kepid kepoi_name   kepler_name koi_disposition\n",
      "0      1  10797460  K00752.01  Kepler-227 b       CONFIRMED\n",
      "1      2  10797460  K00752.02  Kepler-227 c       CONFIRMED\n",
      "2      3  10811496  K00753.01           NaN       CANDIDATE\n"
     ]
    }
   ],
   "source": [
    "# Get basic info about both datasets\n",
    "print(\"K2 DATASET INFO:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Shape: {k2_df.shape}\")\n",
    "print(f\"Columns: {k2_df.shape[1]}\")\n",
    "print(\"Sample columns:\", k2_df.columns.tolist()[:10])\n",
    "\n",
    "print(\"\\nKEPLER DATASET INFO:\")\n",
    "print(\"-\" * 20)\n",
    "print(f\"Shape: {kepler_df.shape}\")\n",
    "print(f\"Columns: {kepler_df.shape[1]}\")\n",
    "print(\"Sample columns:\", kepler_df.columns.tolist()[:10])\n",
    "\n",
    "# Check for common identifier columns\n",
    "potential_ids = ['pl_name', 'hostname', 'kepid', 'kepoi_name', 'k2_name', 'epic_hostname']\n",
    "common_ids = []\n",
    "for id_col in potential_ids:\n",
    "    if id_col in k2_df.columns and id_col in kepler_df.columns:\n",
    "        common_ids.append(id_col)\n",
    "    elif id_col in k2_df.columns:\n",
    "        print(f\"'{id_col}' found in K2 only\")\n",
    "    elif id_col in kepler_df.columns:\n",
    "        print(f\"'{id_col}' found in Kepler only\")\n",
    "\n",
    "print(f\"\\nCommon identifier columns: {common_ids}\")\n",
    "\n",
    "# Quick look at the first few rows to understand data structure\n",
    "print(f\"\\nK2 first few columns preview:\")\n",
    "print(k2_df.iloc[:3, :5])\n",
    "\n",
    "print(f\"\\nKepler first few columns preview:\")\n",
    "print(kepler_df.iloc[:3, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c081018e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created mapping for 51 unified columns\n",
      "Sample mappings:\n",
      "  planet_name: K2='pl_name', Kepler='kepler_name'\n",
      "  host_star_name: K2='hostname', Kepler='None'\n",
      "  kepid: K2='None', Kepler='kepid'\n",
      "  koi_name: K2='None', Kepler='kepoi_name'\n",
      "  k2_name: K2='k2_name', Kepler='None'\n",
      "  disposition: K2='disposition', Kepler='koi_disposition'\n",
      "  disposition_reference: K2='disp_refname', Kepler='None'\n",
      "  project_disposition: K2='None', Kepler='koi_pdisposition'\n",
      "  disposition_score: K2='None', Kepler='koi_score'\n",
      "  num_stars: K2='sy_snum', Kepler='None'\n",
      "\n",
      "Columns available in K2 only: 14\n",
      "Columns available in Kepler only: 8\n",
      "Columns available in both: 29\n"
     ]
    }
   ],
   "source": [
    "# Create comprehensive column mapping based on documentation\n",
    "# Mapping similar columns from both datasets to unified names\n",
    "\n",
    "column_mapping = {\n",
    "    # Planet identification\n",
    "    'planet_name': {'k2': 'pl_name', 'kepler': 'kepler_name'},\n",
    "    'host_star_name': {'k2': 'hostname', 'kepler': None},\n",
    "    'kepid': {'k2': None, 'kepler': 'kepid'},\n",
    "    'koi_name': {'k2': None, 'kepler': 'kepoi_name'},\n",
    "    'k2_name': {'k2': 'k2_name', 'kepler': None},\n",
    "    \n",
    "    # Disposition and status\n",
    "    'disposition': {'k2': 'disposition', 'kepler': 'koi_disposition'},\n",
    "    'disposition_reference': {'k2': 'disp_refname', 'kepler': None},\n",
    "    'project_disposition': {'k2': None, 'kepler': 'koi_pdisposition'},\n",
    "    'disposition_score': {'k2': None, 'kepler': 'koi_score'},\n",
    "    \n",
    "    # System properties\n",
    "    'num_stars': {'k2': 'sy_snum', 'kepler': None},\n",
    "    'num_planets': {'k2': 'sy_pnum', 'kepler': None},\n",
    "    \n",
    "    # Discovery information\n",
    "    'discovery_method': {'k2': 'discoverymethod', 'kepler': None},\n",
    "    'discovery_year': {'k2': 'disc_year', 'kepler': None},\n",
    "    'discovery_facility': {'k2': 'disc_facility', 'kepler': None},\n",
    "    \n",
    "    # Orbital parameters\n",
    "    'orbital_period': {'k2': 'pl_orbper', 'kepler': 'koi_period'},\n",
    "    'orbital_period_err1': {'k2': 'pl_orbpererr1', 'kepler': 'koi_period_err1'},\n",
    "    'orbital_period_err2': {'k2': 'pl_orbpererr2', 'kepler': 'koi_period_err2'},\n",
    "    'semi_major_axis': {'k2': 'pl_orbsmax', 'kepler': 'koi_sma'},\n",
    "    'eccentricity': {'k2': 'pl_orbeccen', 'kepler': 'koi_eccen'},\n",
    "    'inclination': {'k2': 'pl_orbincl', 'kepler': 'koi_incl'},\n",
    "    \n",
    "    # Planet physical properties\n",
    "    'planet_radius_earth': {'k2': 'pl_rade', 'kepler': 'koi_prad'},\n",
    "    'planet_radius_earth_err1': {'k2': 'pl_radeerr1', 'kepler': 'koi_prad_err1'},\n",
    "    'planet_radius_earth_err2': {'k2': 'pl_radeerr2', 'kepler': 'koi_prad_err2'},\n",
    "    'planet_radius_jupiter': {'k2': 'pl_radj', 'kepler': None},\n",
    "    'planet_mass_earth': {'k2': 'pl_masse', 'kepler': None},\n",
    "    'planet_mass_jupiter': {'k2': 'pl_massj', 'kepler': None},\n",
    "    'equilibrium_temperature': {'k2': 'pl_eqt', 'kepler': 'koi_teq'},\n",
    "    'insolation_flux': {'k2': 'pl_insol', 'kepler': 'koi_insol'},\n",
    "    \n",
    "    # Transit properties\n",
    "    'transit_epoch': {'k2': 'pl_tranmid', 'kepler': 'koi_time0bk'},\n",
    "    'transit_duration': {'k2': 'pl_trandur', 'kepler': 'koi_duration'},\n",
    "    'transit_depth': {'k2': 'pl_trandep', 'kepler': 'koi_depth'},\n",
    "    'impact_parameter': {'k2': 'pl_imppar', 'kepler': 'koi_impact'},\n",
    "    \n",
    "    # Stellar properties\n",
    "    'stellar_effective_temp': {'k2': 'st_teff', 'kepler': 'koi_steff'},\n",
    "    'stellar_radius': {'k2': 'st_rad', 'kepler': 'koi_srad'},\n",
    "    'stellar_mass': {'k2': 'st_mass', 'kepler': 'koi_smass'},\n",
    "    'stellar_metallicity': {'k2': 'st_met', 'kepler': 'koi_smet'},\n",
    "    'stellar_surface_gravity': {'k2': 'st_logg', 'kepler': 'koi_slogg'},\n",
    "    'stellar_age': {'k2': 'st_age', 'kepler': 'koi_sage'},\n",
    "    \n",
    "    # Photometry\n",
    "    'kepler_magnitude': {'k2': 'sy_kepmag', 'kepler': 'koi_kepmag'},\n",
    "    'v_magnitude': {'k2': 'sy_vmag', 'kepler': None},\n",
    "    'j_magnitude': {'k2': 'sy_jmag', 'kepler': 'koi_jmag'},\n",
    "    'h_magnitude': {'k2': 'sy_hmag', 'kepler': 'koi_hmag'},\n",
    "    'k_magnitude': {'k2': 'sy_kmag', 'kepler': 'koi_kmag'},\n",
    "    'gaia_magnitude': {'k2': 'sy_gaiamag', 'kepler': None},\n",
    "    \n",
    "    # Position\n",
    "    'ra': {'k2': 'ra', 'kepler': 'ra'},\n",
    "    'dec': {'k2': 'dec', 'kepler': 'dec'},\n",
    "    'distance': {'k2': 'sy_dist', 'kepler': None},\n",
    "    \n",
    "    # Additional Kepler-specific columns that might be useful\n",
    "    'max_single_event_stat': {'k2': None, 'kepler': 'koi_max_sngle_ev'},\n",
    "    'max_multiple_event_stat': {'k2': None, 'kepler': 'koi_max_mult_ev'},\n",
    "    'signal_to_noise': {'k2': None, 'kepler': 'koi_model_snr'},\n",
    "    'num_transits': {'k2': None, 'kepler': 'koi_num_transits'},\n",
    "}\n",
    "\n",
    "print(f\"Created mapping for {len(column_mapping)} unified columns\")\n",
    "print(\"Sample mappings:\")\n",
    "for i, (unified_name, mapping) in enumerate(list(column_mapping.items())[:10]):\n",
    "    print(f\"  {unified_name}: K2='{mapping['k2']}', Kepler='{mapping['kepler']}'\")\n",
    "    \n",
    "print(f\"\\nColumns available in K2 only: {sum(1 for m in column_mapping.values() if m['k2'] and not m['kepler'])}\")\n",
    "print(f\"Columns available in Kepler only: {sum(1 for m in column_mapping.values() if m['kepler'] and not m['k2'])}\")\n",
    "print(f\"Columns available in both: {sum(1 for m in column_mapping.values() if m['k2'] and m['kepler'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b9cabb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating unified K2 dataframe...\n",
      "K2 unified shape: (4004, 52)\n",
      "\n",
      "Creating unified Kepler dataframe...\n",
      "Kepler unified shape: (9564, 52)\n",
      "\n",
      "Data availability summary:\n",
      "----------------------------------------\n",
      "Columns with data in both datasets: 20\n",
      "Columns with data only in K2: 11\n",
      "Columns with data only in Kepler: 16\n",
      "Columns with no data: 4\n",
      "\n",
      "Sample columns available in both:\n",
      "  - planet_name\n",
      "  - disposition\n",
      "  - orbital_period\n",
      "  - orbital_period_err1\n",
      "  - orbital_period_err2\n",
      "  - semi_major_axis\n",
      "  - eccentricity\n",
      "  - planet_radius_earth\n",
      "  - planet_radius_earth_err1\n",
      "  - planet_radius_earth_err2\n",
      "\n",
      "Sample K2-only columns:\n",
      "  - host_star_name\n",
      "  - disposition_reference\n",
      "  - num_stars\n",
      "  - num_planets\n",
      "  - discovery_method\n",
      "\n",
      "Sample Kepler-only columns:\n",
      "  - kepid\n",
      "  - koi_name\n",
      "  - project_disposition\n",
      "  - disposition_score\n",
      "  - inclination\n"
     ]
    }
   ],
   "source": [
    "def create_unified_dataframe(df, dataset_name, column_mapping):\n",
    "    \"\"\"\n",
    "    Create a unified dataframe using the column mapping\n",
    "    \"\"\"\n",
    "    unified_df = pd.DataFrame()\n",
    "    \n",
    "    # Add dataset source identifier\n",
    "    unified_df['data_source'] = dataset_name\n",
    "    \n",
    "    # Map columns according to the mapping\n",
    "    for unified_col, mapping in column_mapping.items():\n",
    "        source_col = mapping[dataset_name.lower()]\n",
    "        if source_col and source_col in df.columns:\n",
    "            unified_df[unified_col] = df[source_col]\n",
    "        else:\n",
    "            unified_df[unified_col] = np.nan\n",
    "    \n",
    "    return unified_df\n",
    "\n",
    "# Create unified dataframes for both datasets\n",
    "print(\"Creating unified K2 dataframe...\")\n",
    "k2_unified = create_unified_dataframe(k2_df, 'K2', column_mapping)\n",
    "print(f\"K2 unified shape: {k2_unified.shape}\")\n",
    "\n",
    "print(\"\\nCreating unified Kepler dataframe...\")\n",
    "kepler_unified = create_unified_dataframe(kepler_df, 'Kepler', column_mapping)\n",
    "print(f\"Kepler unified shape: {kepler_unified.shape}\")\n",
    "\n",
    "# Check which columns have data in each dataset\n",
    "print(\"\\nData availability summary:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "k2_available = []\n",
    "kepler_available = []\n",
    "both_available = []\n",
    "neither_available = []\n",
    "\n",
    "for col in column_mapping.keys():\n",
    "    k2_has_data = not k2_unified[col].isna().all()\n",
    "    kepler_has_data = not kepler_unified[col].isna().all()\n",
    "    \n",
    "    if k2_has_data and kepler_has_data:\n",
    "        both_available.append(col)\n",
    "    elif k2_has_data:\n",
    "        k2_available.append(col)\n",
    "    elif kepler_has_data:\n",
    "        kepler_available.append(col)\n",
    "    else:\n",
    "        neither_available.append(col)\n",
    "\n",
    "print(f\"Columns with data in both datasets: {len(both_available)}\")\n",
    "print(f\"Columns with data only in K2: {len(k2_available)}\")\n",
    "print(f\"Columns with data only in Kepler: {len(kepler_available)}\")\n",
    "print(f\"Columns with no data: {len(neither_available)}\")\n",
    "\n",
    "# Show sample of available columns\n",
    "print(f\"\\nSample columns available in both:\")\n",
    "for col in both_available[:10]:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nSample K2-only columns:\")\n",
    "for col in k2_available[:5]:\n",
    "    print(f\"  - {col}\")\n",
    "    \n",
    "print(f\"\\nSample Kepler-only columns:\")\n",
    "for col in kepler_available[:5]:\n",
    "    print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b7bbce0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANING AND STANDARDIZING DATA\n",
      "========================================\n",
      "Cleaning K2 data...\n",
      "K2 cleaning completed:\n",
      "  - Rows: 4004\n",
      "  - Columns with data: 31\n",
      "\n",
      "Cleaning Kepler data...\n",
      "Kepler cleaning completed:\n",
      "  - Rows: 9564\n",
      "  - Columns with data: 36\n",
      "\n",
      "DATA QUALITY CHECK:\n",
      "-------------------------\n",
      "K2 duplicate rows: 28\n",
      "Kepler duplicate rows: 0\n",
      "\n",
      "Disposition distribution:\n",
      "K2:\n",
      "disposition\n",
      "CONFIRMED         2315\n",
      "CANDIDATE         1374\n",
      "FALSE POSITIVE     315\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Kepler:\n",
      "disposition\n",
      "FALSE POSITIVE    4839\n",
      "CONFIRMED         2746\n",
      "CANDIDATE         1979\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and standardization\n",
    "print(\"CLEANING AND STANDARDIZING DATA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Check data types and clean up\n",
    "def clean_dataframe(df, dataset_name):\n",
    "    \"\"\"Clean and standardize the dataframe\"\"\"\n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # Convert numeric columns that might be stored as strings\n",
    "    numeric_columns = [\n",
    "        'orbital_period', 'orbital_period_err1', 'orbital_period_err2',\n",
    "        'semi_major_axis', 'eccentricity', 'inclination',\n",
    "        'planet_radius_earth', 'planet_radius_earth_err1', 'planet_radius_earth_err2',\n",
    "        'equilibrium_temperature', 'insolation_flux',\n",
    "        'transit_epoch', 'transit_duration', 'transit_depth', 'impact_parameter',\n",
    "        'stellar_effective_temp', 'stellar_radius', 'stellar_mass',\n",
    "        'stellar_metallicity', 'stellar_surface_gravity', 'stellar_age',\n",
    "        'kepler_magnitude', 'v_magnitude', 'j_magnitude', 'h_magnitude', 'k_magnitude',\n",
    "        'ra', 'dec', 'distance', 'max_single_event_stat', 'max_multiple_event_stat',\n",
    "        'signal_to_noise', 'num_transits'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = pd.to_numeric(df_clean[col], errors='coerce')\n",
    "    \n",
    "    # Standardize disposition values\n",
    "    if 'disposition' in df_clean.columns:\n",
    "        disposition_mapping = {\n",
    "            'CONFIRMED': 'CONFIRMED',\n",
    "            'CANDIDATE': 'CANDIDATE', \n",
    "            'FALSE POSITIVE': 'FALSE POSITIVE',\n",
    "            'REFUTED': 'FALSE POSITIVE',  # Standardize refuted to false positive\n",
    "            'NOT DISPOSITIONED': 'NOT DISPOSITIONED'\n",
    "        }\n",
    "        df_clean['disposition'] = df_clean['disposition'].map(disposition_mapping).fillna(df_clean['disposition'])\n",
    "    \n",
    "    print(f\"{dataset_name} cleaning completed:\")\n",
    "    print(f\"  - Rows: {len(df_clean)}\")\n",
    "    print(f\"  - Columns with data: {sum(~df_clean.isna().all())}\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Clean both datasets\n",
    "print(\"Cleaning K2 data...\")\n",
    "k2_clean = clean_dataframe(k2_unified, \"K2\")\n",
    "\n",
    "print(\"\\nCleaning Kepler data...\")\n",
    "kepler_clean = clean_dataframe(kepler_unified, \"Kepler\")\n",
    "\n",
    "# Check for any data quality issues\n",
    "print(f\"\\nDATA QUALITY CHECK:\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Check for duplicates within each dataset\n",
    "k2_dupes = k2_clean.duplicated().sum()\n",
    "kepler_dupes = kepler_clean.duplicated().sum()\n",
    "print(f\"K2 duplicate rows: {k2_dupes}\")\n",
    "print(f\"Kepler duplicate rows: {kepler_dupes}\")\n",
    "\n",
    "# Check disposition distribution\n",
    "print(f\"\\nDisposition distribution:\")\n",
    "if 'disposition' in k2_clean.columns:\n",
    "    print(\"K2:\")\n",
    "    print(k2_clean['disposition'].value_counts())\n",
    "    \n",
    "if 'disposition' in kepler_clean.columns:\n",
    "    print(\"\\nKepler:\")\n",
    "    print(kepler_clean['disposition'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4013a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MERGING DATASETS\n",
      "====================\n",
      "Master dataset created!\n",
      "Total rows: 13568\n",
      "Total columns: 52\n",
      "\n",
      "Adding computed columns...\n",
      "Dataset organization completed!\n",
      "Final shape: (13568, 55)\n",
      "\n",
      "Data source distribution:\n",
      "\n",
      "Combined disposition distribution:\n",
      "  FALSE POSITIVE: 5,154 objects\n",
      "  CONFIRMED: 5,061 objects\n",
      "  CANDIDATE: 3,353 objects\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets\n",
    "print(\"MERGING DATASETS\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "# Combine both cleaned datasets\n",
    "master_df = pd.concat([k2_clean, kepler_clean], ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"Master dataset created!\")\n",
    "print(f\"Total rows: {len(master_df)}\")\n",
    "print(f\"Total columns: {len(master_df.columns)}\")\n",
    "\n",
    "# Add some useful computed columns\n",
    "print(f\"\\nAdding computed columns...\")\n",
    "\n",
    "# Create a unique identifier for tracking\n",
    "master_df['row_id'] = range(1, len(master_df) + 1)\n",
    "\n",
    "# Add mission information based on data source\n",
    "master_df['mission'] = master_df['data_source'].map({\n",
    "    'K2': 'K2',\n",
    "    'Kepler': 'Kepler'\n",
    "})\n",
    "\n",
    "# Create a combined identifier when possible\n",
    "def create_combined_id(row):\n",
    "    if pd.notna(row['planet_name']):\n",
    "        return row['planet_name']\n",
    "    elif pd.notna(row['koi_name']):\n",
    "        return row['koi_name']\n",
    "    elif pd.notna(row['k2_name']):\n",
    "        return row['k2_name']\n",
    "    else:\n",
    "        return f\"{row['data_source']}_{row['row_id']}\"\n",
    "\n",
    "master_df['combined_id'] = master_df.apply(create_combined_id, axis=1)\n",
    "\n",
    "# Reorder columns for better organization\n",
    "priority_columns = [\n",
    "    'row_id', 'combined_id', 'data_source', 'mission',\n",
    "    'planet_name', 'host_star_name', 'kepid', 'koi_name', 'k2_name',\n",
    "    'disposition', 'project_disposition', 'disposition_score',\n",
    "    'orbital_period', 'semi_major_axis', 'eccentricity', 'inclination',\n",
    "    'planet_radius_earth', 'equilibrium_temperature', 'insolation_flux',\n",
    "    'transit_duration', 'transit_depth', 'impact_parameter',\n",
    "    'stellar_effective_temp', 'stellar_radius', 'stellar_mass',\n",
    "    'kepler_magnitude', 'ra', 'dec'\n",
    "]\n",
    "\n",
    "# Get remaining columns\n",
    "remaining_columns = [col for col in master_df.columns if col not in priority_columns]\n",
    "\n",
    "# Reorder\n",
    "final_column_order = priority_columns + remaining_columns\n",
    "master_df = master_df[final_column_order]\n",
    "\n",
    "print(f\"Dataset organization completed!\")\n",
    "print(f\"Final shape: {master_df.shape}\")\n",
    "\n",
    "# Save information about data sources\n",
    "data_source_summary = master_df['data_source'].value_counts()\n",
    "print(f\"\\nData source distribution:\")\n",
    "for source, count in data_source_summary.items():\n",
    "    print(f\"  {source}: {count:,} objects\")\n",
    "\n",
    "# Check disposition distribution in merged dataset\n",
    "print(f\"\\nCombined disposition distribution:\")\n",
    "disposition_summary = master_df['disposition'].value_counts()\n",
    "for disp, count in disposition_summary.items():\n",
    "    print(f\"  {disp}: {count:,} objects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3190fb36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MASTER DATASET VALIDATION & SUMMARY\n",
      "========================================\n",
      "📊 DATASET OVERVIEW:\n",
      "   Total objects: 13,568\n",
      "   Total features: 55\n",
      "   K2 objects: 0\n",
      "   Kepler objects: 0\n",
      "\n",
      "📈 DATA COMPLETENESS (% of non-null values):\n",
      "   planet_name: 49.8%\n",
      "   disposition: 100.0%\n",
      "   orbital_period: 99.7%\n",
      "   planet_radius_earth: 91.2%\n",
      "   stellar_effective_temp: 89.1%\n",
      "   kepler_magnitude: 70.5%\n",
      "   ra: 100.0%\n",
      "   dec: 100.0%\n",
      "\n",
      "📋 SAMPLE DATA (first 5 rows):\n",
      "         combined_id data_source disposition  orbital_period  \\\n",
      "0  EPIC 201111557.01         NaN   CANDIDATE        2.301830   \n",
      "1  EPIC 201111557.01         NaN   CANDIDATE        2.302368   \n",
      "2  EPIC 201126503.01         NaN   CANDIDATE        1.194749   \n",
      "3  EPIC 201127519.01         NaN   CANDIDATE        6.178369   \n",
      "4  EPIC 201127519.01         NaN   CANDIDATE        6.178870   \n",
      "\n",
      "   planet_radius_earth  stellar_effective_temp  \n",
      "0             1.120000                 4616.52  \n",
      "1             1.312588                 4720.00  \n",
      "2             4.190000                 3919.00  \n",
      "3             9.912579                 5015.00  \n",
      "4             8.840000                 4719.28  \n",
      "\n",
      "📊 NUMERICAL SUMMARY:\n",
      "       orbital_period  planet_radius_earth  equilibrium_temperature  \\\n",
      "count       13524.000            12374.000                10055.000   \n",
      "mean           65.302               78.678                 1070.310   \n",
      "std          1337.142             2654.200                  830.872   \n",
      "min             0.176                0.080                   25.000   \n",
      "25%             2.791                1.500                  546.000   \n",
      "50%             8.328                2.470                  871.000   \n",
      "75%            24.948               10.495                 1352.000   \n",
      "max        129995.778           200346.000                14667.000   \n",
      "\n",
      "       stellar_effective_temp  stellar_radius  stellar_mass  \n",
      "count               12093.000       13075.000     11303.000  \n",
      "mean                 5569.687           1.565         0.996  \n",
      "std                   951.502           5.310         0.370  \n",
      "min                  2520.000           0.109         0.000  \n",
      "25%                  5157.000           0.789         0.822  \n",
      "50%                  5662.000           0.966         0.960  \n",
      "75%                  6063.000           1.289         1.091  \n",
      "max                 46696.000         229.908        14.336  \n",
      "\n",
      "🔍 OVERLAP ANALYSIS:\n",
      "   Potential overlapping planet names: 0\n",
      "\n",
      "📋 COLUMN AVAILABILITY BY SOURCE:\n",
      "   K2: 0 columns with data\n",
      "   Kepler: 0 columns with data\n",
      "\n",
      "✅ MASTER DATASET SUCCESSFULLY CREATED!\n",
      "   Ready for analysis with 13,568 exoplanet objects\n",
      "   Contains data from both K2 and Kepler missions\n",
      "   Unified schema with 55 total features\n"
     ]
    }
   ],
   "source": [
    "# Final validation and summary\n",
    "print(\"MASTER DATASET VALIDATION & SUMMARY\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"📊 DATASET OVERVIEW:\")\n",
    "print(f\"   Total objects: {len(master_df):,}\")\n",
    "print(f\"   Total features: {len(master_df.columns)}\")\n",
    "print(f\"   K2 objects: {len(master_df[master_df['data_source'] == 'K2']):,}\")\n",
    "print(f\"   Kepler objects: {len(master_df[master_df['data_source'] == 'Kepler']):,}\")\n",
    "\n",
    "# Check data completeness for key columns\n",
    "print(f\"\\n📈 DATA COMPLETENESS (% of non-null values):\")\n",
    "key_columns = [\n",
    "    'planet_name', 'disposition', 'orbital_period', 'planet_radius_earth',\n",
    "    'stellar_effective_temp', 'kepler_magnitude', 'ra', 'dec'\n",
    "]\n",
    "\n",
    "for col in key_columns:\n",
    "    if col in master_df.columns:\n",
    "        completeness = (1 - master_df[col].isna().sum() / len(master_df)) * 100\n",
    "        print(f\"   {col}: {completeness:.1f}%\")\n",
    "\n",
    "# Show sample data\n",
    "print(f\"\\n📋 SAMPLE DATA (first 5 rows):\")\n",
    "sample_cols = ['combined_id', 'data_source', 'disposition', 'orbital_period', \n",
    "               'planet_radius_earth', 'stellar_effective_temp']\n",
    "available_sample_cols = [col for col in sample_cols if col in master_df.columns]\n",
    "print(master_df[available_sample_cols].head())\n",
    "\n",
    "# Summary statistics for numerical columns\n",
    "print(f\"\\n📊 NUMERICAL SUMMARY:\")\n",
    "numerical_cols = ['orbital_period', 'planet_radius_earth', 'equilibrium_temperature', \n",
    "                  'stellar_effective_temp', 'stellar_radius', 'stellar_mass']\n",
    "available_numerical_cols = [col for col in numerical_cols if col in master_df.columns]\n",
    "\n",
    "if available_numerical_cols:\n",
    "    summary_stats = master_df[available_numerical_cols].describe()\n",
    "    print(summary_stats.round(3))\n",
    "\n",
    "# Check for potential overlaps between datasets\n",
    "print(f\"\\n🔍 OVERLAP ANALYSIS:\")\n",
    "k2_planets = set(master_df[master_df['data_source'] == 'K2']['planet_name'].dropna())\n",
    "kepler_planets = set(master_df[master_df['data_source'] == 'Kepler']['planet_name'].dropna())\n",
    "overlapping_names = k2_planets.intersection(kepler_planets)\n",
    "print(f\"   Potential overlapping planet names: {len(overlapping_names)}\")\n",
    "if overlapping_names:\n",
    "    print(f\"   Examples: {list(overlapping_names)[:5]}\")\n",
    "\n",
    "# Column availability summary\n",
    "print(f\"\\n📋 COLUMN AVAILABILITY BY SOURCE:\")\n",
    "for source in ['K2', 'Kepler']:\n",
    "    source_data = master_df[master_df['data_source'] == source]\n",
    "    available_cols = sum(~source_data.isna().all())\n",
    "    print(f\"   {source}: {available_cols} columns with data\")\n",
    "\n",
    "print(f\"\\n✅ MASTER DATASET SUCCESSFULLY CREATED!\")\n",
    "print(f\"   Ready for analysis with {len(master_df):,} exoplanet objects\")\n",
    "print(f\"   Contains data from both K2 and Kepler missions\")\n",
    "print(f\"   Unified schema with {len(master_df.columns)} total features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "eccecef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 FINAL MASTER DATASET INFORMATION\n",
      "=============================================\n",
      "Dataset Name: Merged Exoplanet Master Dataset\n",
      "Sources: K2 Planets & Candidates + Kepler Cumulative\n",
      "Total Records: 13,568\n",
      "Total Features: 55\n",
      "\n",
      "📊 Record Distribution:\n",
      "   K2 Mission: 0 objects\n",
      "   Kepler Mission: 0 objects\n",
      "\n",
      "🏷️ All Available Columns:\n",
      " 1. row_id\n",
      " 2. combined_id\n",
      " 3. data_source\n",
      " 4. mission\n",
      " 5. planet_name\n",
      " 6. host_star_name\n",
      " 7. kepid\n",
      " 8. koi_name\n",
      " 9. k2_name\n",
      "10. disposition\n",
      "11. project_disposition\n",
      "12. disposition_score\n",
      "13. orbital_period\n",
      "14. semi_major_axis\n",
      "15. eccentricity\n",
      "16. inclination\n",
      "17. planet_radius_earth\n",
      "18. equilibrium_temperature\n",
      "19. insolation_flux\n",
      "20. transit_duration\n",
      "21. transit_depth\n",
      "22. impact_parameter\n",
      "23. stellar_effective_temp\n",
      "24. stellar_radius\n",
      "25. stellar_mass\n",
      "26. kepler_magnitude\n",
      "27. ra\n",
      "28. dec\n",
      "29. disposition_reference\n",
      "30. num_stars\n",
      "31. num_planets\n",
      "32. discovery_method\n",
      "33. discovery_year\n",
      "34. discovery_facility\n",
      "35. orbital_period_err1\n",
      "36. orbital_period_err2\n",
      "37. planet_radius_earth_err1\n",
      "38. planet_radius_earth_err2\n",
      "39. planet_radius_jupiter\n",
      "40. planet_mass_earth\n",
      "41. planet_mass_jupiter\n",
      "42. transit_epoch\n",
      "43. stellar_metallicity\n",
      "44. stellar_surface_gravity\n",
      "45. stellar_age\n",
      "46. v_magnitude\n",
      "47. j_magnitude\n",
      "48. h_magnitude\n",
      "49. k_magnitude\n",
      "50. gaia_magnitude\n",
      "51. distance\n",
      "52. max_single_event_stat\n",
      "53. max_multiple_event_stat\n",
      "54. signal_to_noise\n",
      "55. num_transits\n",
      "\n",
      "🔍 Sample Records:\n",
      "================================================================================\n",
      "      combined_id data_source disposition  orbital_period  planet_radius_earth  stellar_effective_temp\n",
      "EPIC 201111557.01         NaN   CANDIDATE        2.301830             1.120000                 4616.52\n",
      "EPIC 201111557.01         NaN   CANDIDATE        2.302368             1.312588                 4720.00\n",
      "EPIC 201126503.01         NaN   CANDIDATE        1.194749             4.190000                 3919.00\n",
      "\n",
      "💾 To save this master dataset:\n",
      "   master_df.to_csv('exoplanet_master_dataset.csv', index=False)\n",
      "   # This will create a unified CSV with all 13,568 objects\n",
      "\n",
      "📋 Data Types Summary:\n",
      "   float64: 43 columns\n",
      "   object: 11 columns\n",
      "   int64: 1 columns\n",
      "\n",
      "✨ SUCCESS: Master dataset ready for machine learning and analysis!\n"
     ]
    }
   ],
   "source": [
    "# Display final dataset info and save option\n",
    "print(\"🎯 FINAL MASTER DATASET INFORMATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "print(f\"Dataset Name: Merged Exoplanet Master Dataset\")\n",
    "print(f\"Sources: K2 Planets & Candidates + Kepler Cumulative\")\n",
    "print(f\"Total Records: {len(master_df):,}\")\n",
    "print(f\"Total Features: {len(master_df.columns)}\")\n",
    "\n",
    "print(f\"\\n📊 Record Distribution:\")\n",
    "print(f\"   K2 Mission: {len(master_df[master_df['data_source'] == 'K2']):,} objects\")\n",
    "print(f\"   Kepler Mission: {len(master_df[master_df['data_source'] == 'Kepler']):,} objects\")\n",
    "\n",
    "print(f\"\\n🏷️ All Available Columns:\")\n",
    "for i, col in enumerate(master_df.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "# Show a few example records to verify structure\n",
    "print(f\"\\n🔍 Sample Records:\")\n",
    "print(\"=\"*80)\n",
    "sample_df = master_df[['combined_id', 'data_source', 'disposition', 'orbital_period', \n",
    "                       'planet_radius_earth', 'stellar_effective_temp']].head(3)\n",
    "print(sample_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\n💾 To save this master dataset:\")\n",
    "print(f\"   master_df.to_csv('exoplanet_master_dataset.csv', index=False)\")\n",
    "print(f\"   # This will create a unified CSV with all {len(master_df):,} objects\")\n",
    "\n",
    "# Quick data type summary\n",
    "print(f\"\\n📋 Data Types Summary:\")\n",
    "dtypes_summary = master_df.dtypes.value_counts()\n",
    "for dtype, count in dtypes_summary.items():\n",
    "    print(f\"   {dtype}: {count} columns\")\n",
    "\n",
    "print(f\"\\n✨ SUCCESS: Master dataset ready for machine learning and analysis!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ca48e452",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>row_id</th>\n",
       "      <th>kepid</th>\n",
       "      <th>k2_name</th>\n",
       "      <th>disposition_score</th>\n",
       "      <th>orbital_period</th>\n",
       "      <th>semi_major_axis</th>\n",
       "      <th>eccentricity</th>\n",
       "      <th>inclination</th>\n",
       "      <th>planet_radius_earth</th>\n",
       "      <th>equilibrium_temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>v_magnitude</th>\n",
       "      <th>j_magnitude</th>\n",
       "      <th>h_magnitude</th>\n",
       "      <th>k_magnitude</th>\n",
       "      <th>gaia_magnitude</th>\n",
       "      <th>distance</th>\n",
       "      <th>max_single_event_stat</th>\n",
       "      <th>max_multiple_event_stat</th>\n",
       "      <th>signal_to_noise</th>\n",
       "      <th>num_transits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>13568.000000</td>\n",
       "      <td>9.564000e+03</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8054.000000</td>\n",
       "      <td>13524.000000</td>\n",
       "      <td>10021.000000</td>\n",
       "      <td>9630.000000</td>\n",
       "      <td>9200.000000</td>\n",
       "      <td>12374.000000</td>\n",
       "      <td>10055.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3962.000000</td>\n",
       "      <td>9539.000000</td>\n",
       "      <td>9539.000000</td>\n",
       "      <td>13520.000000</td>\n",
       "      <td>3948.000000</td>\n",
       "      <td>3879.000000</td>\n",
       "      <td>8422.000000</td>\n",
       "      <td>8422.000000</td>\n",
       "      <td>9201.000000</td>\n",
       "      <td>8422.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>6784.500000</td>\n",
       "      <td>7.690628e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.480829</td>\n",
       "      <td>65.302494</td>\n",
       "      <td>0.214238</td>\n",
       "      <td>0.005379</td>\n",
       "      <td>82.469147</td>\n",
       "      <td>78.677536</td>\n",
       "      <td>1070.309560</td>\n",
       "      <td>...</td>\n",
       "      <td>13.174241</td>\n",
       "      <td>12.993311</td>\n",
       "      <td>12.620604</td>\n",
       "      <td>12.003345</td>\n",
       "      <td>12.837391</td>\n",
       "      <td>391.104905</td>\n",
       "      <td>176.846052</td>\n",
       "      <td>1025.664672</td>\n",
       "      <td>259.895001</td>\n",
       "      <td>385.006768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3916.888561</td>\n",
       "      <td>2.653459e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.476928</td>\n",
       "      <td>1337.141748</td>\n",
       "      <td>0.549591</td>\n",
       "      <td>0.039040</td>\n",
       "      <td>15.223627</td>\n",
       "      <td>2654.199983</td>\n",
       "      <td>830.871608</td>\n",
       "      <td>...</td>\n",
       "      <td>1.900071</td>\n",
       "      <td>1.291912</td>\n",
       "      <td>1.267215</td>\n",
       "      <td>1.555375</td>\n",
       "      <td>1.737748</td>\n",
       "      <td>543.808755</td>\n",
       "      <td>770.902357</td>\n",
       "      <td>4154.121620</td>\n",
       "      <td>795.806615</td>\n",
       "      <td>545.756200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7.574500e+05</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.175660</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.290000</td>\n",
       "      <td>0.080000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>5.840000</td>\n",
       "      <td>4.097000</td>\n",
       "      <td>3.014000</td>\n",
       "      <td>2.311000</td>\n",
       "      <td>5.808980</td>\n",
       "      <td>21.818200</td>\n",
       "      <td>2.417437</td>\n",
       "      <td>7.105086</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>3392.750000</td>\n",
       "      <td>5.556034e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.790824</td>\n",
       "      <td>0.038170</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>83.920000</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>546.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.059750</td>\n",
       "      <td>12.253000</td>\n",
       "      <td>11.914500</td>\n",
       "      <td>10.995000</td>\n",
       "      <td>11.803625</td>\n",
       "      <td>156.110500</td>\n",
       "      <td>3.997856</td>\n",
       "      <td>10.733030</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>41.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>6784.500000</td>\n",
       "      <td>7.906892e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.334000</td>\n",
       "      <td>8.328175</td>\n",
       "      <td>0.081800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>88.500000</td>\n",
       "      <td>2.470000</td>\n",
       "      <td>871.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>12.975500</td>\n",
       "      <td>13.236000</td>\n",
       "      <td>12.834000</td>\n",
       "      <td>12.226000</td>\n",
       "      <td>12.670200</td>\n",
       "      <td>264.780000</td>\n",
       "      <td>5.589751</td>\n",
       "      <td>19.254411</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>143.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>10176.250000</td>\n",
       "      <td>9.873066e+06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.998000</td>\n",
       "      <td>24.948485</td>\n",
       "      <td>0.195800</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>89.770000</td>\n",
       "      <td>10.495000</td>\n",
       "      <td>1352.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>14.314000</td>\n",
       "      <td>13.968000</td>\n",
       "      <td>13.551000</td>\n",
       "      <td>13.238000</td>\n",
       "      <td>13.917550</td>\n",
       "      <td>446.821000</td>\n",
       "      <td>16.947631</td>\n",
       "      <td>71.998003</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>469.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>13568.000000</td>\n",
       "      <td>1.293514e+07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>129995.778400</td>\n",
       "      <td>44.989200</td>\n",
       "      <td>0.853000</td>\n",
       "      <td>90.000000</td>\n",
       "      <td>200346.000000</td>\n",
       "      <td>14667.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.556100</td>\n",
       "      <td>17.372000</td>\n",
       "      <td>17.615000</td>\n",
       "      <td>17.038000</td>\n",
       "      <td>20.247800</td>\n",
       "      <td>9319.510000</td>\n",
       "      <td>22982.162000</td>\n",
       "      <td>120049.680000</td>\n",
       "      <td>9054.700000</td>\n",
       "      <td>2664.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             row_id         kepid  k2_name  disposition_score  orbital_period  \\\n",
       "count  13568.000000  9.564000e+03      0.0        8054.000000    13524.000000   \n",
       "mean    6784.500000  7.690628e+06      NaN           0.480829       65.302494   \n",
       "std     3916.888561  2.653459e+06      NaN           0.476928     1337.141748   \n",
       "min        1.000000  7.574500e+05      NaN           0.000000        0.175660   \n",
       "25%     3392.750000  5.556034e+06      NaN           0.000000        2.790824   \n",
       "50%     6784.500000  7.906892e+06      NaN           0.334000        8.328175   \n",
       "75%    10176.250000  9.873066e+06      NaN           0.998000       24.948485   \n",
       "max    13568.000000  1.293514e+07      NaN           1.000000   129995.778400   \n",
       "\n",
       "       semi_major_axis  eccentricity  inclination  planet_radius_earth  \\\n",
       "count     10021.000000   9630.000000  9200.000000         12374.000000   \n",
       "mean          0.214238      0.005379    82.469147            78.677536   \n",
       "std           0.549591      0.039040    15.223627          2654.199983   \n",
       "min           0.003400      0.000000     2.290000             0.080000   \n",
       "25%           0.038170      0.000000    83.920000             1.500000   \n",
       "50%           0.081800      0.000000    88.500000             2.470000   \n",
       "75%           0.195800      0.000000    89.770000            10.495000   \n",
       "max          44.989200      0.853000    90.000000        200346.000000   \n",
       "\n",
       "       equilibrium_temperature  ...  v_magnitude  j_magnitude  h_magnitude  \\\n",
       "count             10055.000000  ...  3962.000000  9539.000000  9539.000000   \n",
       "mean               1070.309560  ...    13.174241    12.993311    12.620604   \n",
       "std                 830.871608  ...     1.900071     1.291912     1.267215   \n",
       "min                  25.000000  ...     5.840000     4.097000     3.014000   \n",
       "25%                 546.000000  ...    12.059750    12.253000    11.914500   \n",
       "50%                 871.000000  ...    12.975500    13.236000    12.834000   \n",
       "75%                1352.000000  ...    14.314000    13.968000    13.551000   \n",
       "max               14667.000000  ...    20.556100    17.372000    17.615000   \n",
       "\n",
       "        k_magnitude  gaia_magnitude     distance  max_single_event_stat  \\\n",
       "count  13520.000000     3948.000000  3879.000000            8422.000000   \n",
       "mean      12.003345       12.837391   391.104905             176.846052   \n",
       "std        1.555375        1.737748   543.808755             770.902357   \n",
       "min        2.311000        5.808980    21.818200               2.417437   \n",
       "25%       10.995000       11.803625   156.110500               3.997856   \n",
       "50%       12.226000       12.670200   264.780000               5.589751   \n",
       "75%       13.238000       13.917550   446.821000              16.947631   \n",
       "max       17.038000       20.247800  9319.510000           22982.162000   \n",
       "\n",
       "       max_multiple_event_stat  signal_to_noise  num_transits  \n",
       "count              8422.000000      9201.000000   8422.000000  \n",
       "mean               1025.664672       259.895001    385.006768  \n",
       "std                4154.121620       795.806615    545.756200  \n",
       "min                   7.105086         0.000000      0.000000  \n",
       "25%                  10.733030        12.000000     41.000000  \n",
       "50%                  19.254411        23.000000    143.000000  \n",
       "75%                  71.998003        78.000000    469.000000  \n",
       "max              120049.680000      9054.700000   2664.000000  \n",
       "\n",
       "[8 rows x 44 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "master_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bee9042d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DISPOSITION CATEGORY ANALYSIS\n",
      "===================================\n",
      "✅ Disposition column found!\n",
      "\n",
      "Disposition value counts:\n",
      "disposition\n",
      "FALSE POSITIVE    5154\n",
      "CONFIRMED         5061\n",
      "CANDIDATE         3353\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Disposition distribution by data source:\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "Sample records with disposition:\n",
      "         combined_id data_source disposition        planet_name koi_name\n",
      "0  EPIC 201111557.01         NaN   CANDIDATE  EPIC 201111557.01      NaN\n",
      "1  EPIC 201111557.01         NaN   CANDIDATE  EPIC 201111557.01      NaN\n",
      "2  EPIC 201126503.01         NaN   CANDIDATE  EPIC 201126503.01      NaN\n",
      "3  EPIC 201127519.01         NaN   CANDIDATE  EPIC 201127519.01      NaN\n",
      "4  EPIC 201127519.01         NaN   CANDIDATE  EPIC 201127519.01      NaN\n",
      "5  EPIC 201147085.01         NaN   CANDIDATE  EPIC 201147085.01      NaN\n",
      "6  EPIC 201152065.01         NaN   CANDIDATE  EPIC 201152065.01      NaN\n",
      "7  EPIC 201160662.01         NaN   CANDIDATE  EPIC 201160662.01      NaN\n",
      "8  EPIC 201164625.01         NaN   CANDIDATE  EPIC 201164625.01      NaN\n",
      "9  EPIC 201166680.01         NaN   CANDIDATE  EPIC 201166680.01      NaN\n",
      "\n",
      "Null disposition values: 0\n"
     ]
    }
   ],
   "source": [
    "# Check disposition category in the master dataset\n",
    "print(\"DISPOSITION CATEGORY ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Check if disposition column exists and show its values\n",
    "if 'disposition' in master_df.columns:\n",
    "    print(\"✅ Disposition column found!\")\n",
    "    print(f\"\\nDisposition value counts:\")\n",
    "    disp_counts = master_df['disposition'].value_counts(dropna=False)\n",
    "    print(disp_counts)\n",
    "    \n",
    "    print(f\"\\nDisposition distribution by data source:\")\n",
    "    disposition_by_source = pd.crosstab(master_df['data_source'], master_df['disposition'], margins=True)\n",
    "    print(disposition_by_source)\n",
    "    \n",
    "    print(f\"\\nSample records with disposition:\")\n",
    "    sample_disp = master_df[['combined_id', 'data_source', 'disposition', 'planet_name', 'koi_name']].head(10)\n",
    "    print(sample_disp)\n",
    "    \n",
    "    # Check for any null values\n",
    "    null_count = master_df['disposition'].isna().sum()\n",
    "    print(f\"\\nNull disposition values: {null_count}\")\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Disposition column not found!\")\n",
    "    print(\"Available columns containing 'disp':\")\n",
    "    disp_columns = [col for col in master_df.columns if 'disp' in col.lower()]\n",
    "    for col in disp_columns:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9b257b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIXING DATA SOURCE AND ANALYZING DISPOSITION\n",
      "=============================================\n",
      "Current data_source values:\n",
      "data_source\n",
      "NaN    13568\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Checking for K2-specific vs Kepler-specific identifiers:\n",
      "Records with K2 identifiers: 1791\n",
      "Records with Kepler identifiers: 9564\n",
      "Records with both: 0\n",
      "Records with neither: 2213\n",
      "\n",
      "Fixed data_source values:\n",
      "data_source\n",
      "Kepler    9564\n",
      "NaN       2213\n",
      "K2        1791\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Disposition distribution by corrected data source:\n",
      "disposition  CANDIDATE  CONFIRMED  FALSE POSITIVE    All\n",
      "data_source                                             \n",
      "K2                1373        125             293   1791\n",
      "Kepler            1979       2746            4839   9564\n",
      "All               3352       2871            5132  11355\n",
      "\n",
      "✅ Master dataframe updated with corrected data sources!\n"
     ]
    }
   ],
   "source": [
    "# Fix the data_source issue and show proper disposition breakdown\n",
    "print(\"FIXING DATA SOURCE AND ANALYZING DISPOSITION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Check the current data_source column\n",
    "print(\"Current data_source values:\")\n",
    "print(master_df['data_source'].value_counts(dropna=False))\n",
    "\n",
    "# Let's check if we can identify K2 vs Kepler records by looking at specific columns\n",
    "print(f\"\\nChecking for K2-specific vs Kepler-specific identifiers:\")\n",
    "\n",
    "# K2 records should have k2_name or EPIC in planet_name\n",
    "k2_mask = (master_df['k2_name'].notna()) | (master_df['planet_name'].str.contains('EPIC', na=False))\n",
    "kepler_mask = (master_df['kepid'].notna()) | (master_df['koi_name'].notna())\n",
    "\n",
    "print(f\"Records with K2 identifiers: {k2_mask.sum()}\")\n",
    "print(f\"Records with Kepler identifiers: {kepler_mask.sum()}\")\n",
    "print(f\"Records with both: {(k2_mask & kepler_mask).sum()}\")\n",
    "print(f\"Records with neither: {(~k2_mask & ~kepler_mask).sum()}\")\n",
    "\n",
    "# Fix the data_source assignment\n",
    "master_df_fixed = master_df.copy()\n",
    "\n",
    "# Assign data sources based on identifiers\n",
    "master_df_fixed.loc[k2_mask & ~kepler_mask, 'data_source'] = 'K2'\n",
    "master_df_fixed.loc[kepler_mask & ~k2_mask, 'data_source'] = 'Kepler'\n",
    "master_df_fixed.loc[k2_mask & kepler_mask, 'data_source'] = 'Both'  # If somehow both\n",
    "\n",
    "print(f\"\\nFixed data_source values:\")\n",
    "print(master_df_fixed['data_source'].value_counts(dropna=False))\n",
    "\n",
    "# Now show disposition by corrected data source\n",
    "print(f\"\\nDisposition distribution by corrected data source:\")\n",
    "if master_df_fixed['data_source'].notna().any():\n",
    "    disposition_by_source_fixed = pd.crosstab(\n",
    "        master_df_fixed['data_source'], \n",
    "        master_df_fixed['disposition'], \n",
    "        margins=True\n",
    "    )\n",
    "    print(disposition_by_source_fixed)\n",
    "\n",
    "# Update the master dataframe\n",
    "master_df = master_df_fixed.copy()\n",
    "print(f\"\\n✅ Master dataframe updated with corrected data sources!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "634a150e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 COMPREHENSIVE DISPOSITION CATEGORY ANALYSIS\n",
      "==================================================\n",
      "Current master_df shape: (13568, 55)\n",
      "Data source distribution:\n",
      "data_source\n",
      "Kepler    9564\n",
      "NaN       2213\n",
      "K2        1791\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔧 RECONSTRUCTING PROPER DATA SOURCES...\n",
      "Original K2 shape: (4004, 94)\n",
      "Original Kepler shape: (9564, 141)\n",
      "\n",
      "🏗️ REBUILDING MASTER DATAFRAME WITH PROPER TRACKING...\n",
      "✅ Rebuilt master dataframe!\n",
      "New shape: (13568, 55)\n",
      "Data source distribution:\n",
      "data_source\n",
      "Kepler    9564\n",
      "K2        4004\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 DISPOSITION ANALYSIS:\n",
      "------------------------------\n",
      "Overall disposition counts:\n",
      "disposition\n",
      "FALSE POSITIVE    5154\n",
      "CONFIRMED         5061\n",
      "CANDIDATE         3353\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Disposition by data source:\n",
      "disposition  CANDIDATE  CONFIRMED  FALSE POSITIVE    All\n",
      "data_source                                             \n",
      "K2                1374       2315             315   4004\n",
      "Kepler            1979       2746            4839   9564\n",
      "All               3353       5061            5154  13568\n",
      "\n",
      "Disposition percentages by source:\n",
      "disposition  CANDIDATE  CONFIRMED  FALSE POSITIVE\n",
      "data_source                                      \n",
      "K2                34.3       57.8             7.9\n",
      "Kepler            20.7       28.7            50.6\n",
      "\n",
      "📋 SAMPLE RECORDS BY SOURCE:\n",
      "K2 Examples:\n",
      "         combined_id disposition        planet_name\n",
      "0  EPIC 201111557.01   CANDIDATE  EPIC 201111557.01\n",
      "1  EPIC 201111557.01   CANDIDATE  EPIC 201111557.01\n",
      "2  EPIC 201126503.01   CANDIDATE  EPIC 201126503.01\n",
      "3  EPIC 201127519.01   CANDIDATE  EPIC 201127519.01\n",
      "4  EPIC 201127519.01   CANDIDATE  EPIC 201127519.01\n",
      "\n",
      "Kepler Examples:\n",
      "       combined_id     disposition   koi_name\n",
      "4004  Kepler-227 b       CONFIRMED  K00752.01\n",
      "4005  Kepler-227 c       CONFIRMED  K00752.02\n",
      "4006     K00753.01       CANDIDATE  K00753.01\n",
      "4007     K00754.01  FALSE POSITIVE  K00754.01\n",
      "4008  Kepler-664 b       CONFIRMED  K00755.01\n"
     ]
    }
   ],
   "source": [
    "# COMPREHENSIVE DISPOSITION ANALYSIS - TRY AGAIN\n",
    "print(\"🎯 COMPREHENSIVE DISPOSITION CATEGORY ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check current state of master_df\n",
    "print(f\"Current master_df shape: {master_df.shape}\")\n",
    "print(f\"Data source distribution:\")\n",
    "print(master_df['data_source'].value_counts(dropna=False))\n",
    "\n",
    "# First, let's properly fix the data source issue by going back to the original approach\n",
    "print(f\"\\n🔧 RECONSTRUCTING PROPER DATA SOURCES...\")\n",
    "\n",
    "# Check what we have in the original dataframes\n",
    "print(f\"Original K2 shape: {k2_df.shape}\")\n",
    "print(f\"Original Kepler shape: {kepler_df.shape}\")\n",
    "\n",
    "# Let's rebuild the master dataframe with proper source tracking\n",
    "print(f\"\\n🏗️ REBUILDING MASTER DATAFRAME WITH PROPER TRACKING...\")\n",
    "\n",
    "# Create K2 subset with source marking\n",
    "k2_master = k2_clean.copy()\n",
    "k2_master['data_source'] = 'K2'\n",
    "k2_master['mission'] = 'K2'\n",
    "\n",
    "# Create Kepler subset with source marking  \n",
    "kepler_master = kepler_clean.copy()\n",
    "kepler_master['data_source'] = 'Kepler'\n",
    "kepler_master['mission'] = 'Kepler'\n",
    "\n",
    "# Combine properly\n",
    "master_df_rebuilt = pd.concat([k2_master, kepler_master], ignore_index=True, sort=False)\n",
    "\n",
    "# Add row IDs and combined IDs\n",
    "master_df_rebuilt['row_id'] = range(1, len(master_df_rebuilt) + 1)\n",
    "\n",
    "def create_combined_id_fixed(row):\n",
    "    if pd.notna(row['planet_name']):\n",
    "        return row['planet_name']\n",
    "    elif pd.notna(row['koi_name']):\n",
    "        return row['koi_name']\n",
    "    elif pd.notna(row['k2_name']):\n",
    "        return row['k2_name']\n",
    "    else:\n",
    "        return f\"{row['data_source']}_{row['row_id']}\"\n",
    "\n",
    "master_df_rebuilt['combined_id'] = master_df_rebuilt.apply(create_combined_id_fixed, axis=1)\n",
    "\n",
    "# Update the global master_df\n",
    "master_df = master_df_rebuilt.copy()\n",
    "\n",
    "print(f\"✅ Rebuilt master dataframe!\")\n",
    "print(f\"New shape: {master_df.shape}\")\n",
    "print(f\"Data source distribution:\")\n",
    "print(master_df['data_source'].value_counts())\n",
    "\n",
    "# NOW ANALYZE DISPOSITION PROPERLY\n",
    "print(f\"\\n📊 DISPOSITION ANALYSIS:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"Overall disposition counts:\")\n",
    "overall_disp = master_df['disposition'].value_counts(dropna=False)\n",
    "print(overall_disp)\n",
    "\n",
    "print(f\"\\nDisposition by data source:\")\n",
    "disp_crosstab = pd.crosstab(master_df['data_source'], master_df['disposition'], margins=True)\n",
    "print(disp_crosstab)\n",
    "\n",
    "print(f\"\\nDisposition percentages by source:\")\n",
    "disp_pct = pd.crosstab(master_df['data_source'], master_df['disposition'], normalize='index') * 100\n",
    "print(disp_pct.round(1))\n",
    "\n",
    "# Show examples from each source\n",
    "print(f\"\\n📋 SAMPLE RECORDS BY SOURCE:\")\n",
    "print(\"K2 Examples:\")\n",
    "k2_sample = master_df[master_df['data_source'] == 'K2'][['combined_id', 'disposition', 'planet_name']].head()\n",
    "print(k2_sample)\n",
    "\n",
    "print(f\"\\nKepler Examples:\")\n",
    "kepler_sample = master_df[master_df['data_source'] == 'Kepler'][['combined_id', 'disposition', 'koi_name']].head()\n",
    "print(kepler_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1605748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 FILTERING DATASET - KEEPING ONLY CANDIDATES AND CONFIRMED PLANETS\n",
      "=================================================================\n",
      "Current disposition distribution:\n",
      "disposition\n",
      "FALSE POSITIVE    5154\n",
      "CONFIRMED         5061\n",
      "CANDIDATE         3353\n",
      "Name: count, dtype: int64\n",
      "Total objects: 13,568\n",
      "\n",
      "✂️ FILTERING RESULTS:\n",
      "-------------------------\n",
      "Objects before filtering: 13,568\n",
      "Objects after filtering: 8,414\n",
      "Objects removed (FALSE POSITIVE): 5,154\n",
      "Retention rate: 62.0%\n",
      "\n",
      "📊 NEW DISPOSITION DISTRIBUTION:\n",
      "-----------------------------------\n",
      "disposition\n",
      "CONFIRMED    5061\n",
      "CANDIDATE    3353\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Distribution by data source:\n",
      "disposition  CANDIDATE  CONFIRMED   All\n",
      "data_source                            \n",
      "K2                1374       2315  3689\n",
      "Kepler            1979       2746  4725\n",
      "All               3353       5061  8414\n",
      "\n",
      "Percentages by source:\n",
      "disposition  CANDIDATE  CONFIRMED\n",
      "data_source                      \n",
      "K2                37.2       62.8\n",
      "Kepler            41.9       58.1\n",
      "\n",
      "✅ DATASET SUCCESSFULLY FILTERED!\n",
      "Final dataset contains 8,414 potential planets (candidates + confirmed)\n",
      "Shape: (8414, 55)\n",
      "\n",
      "📋 SAMPLE OF FILTERED DATA:\n",
      "   row_id        combined_id data_source disposition  planet_radius_earth  \\\n",
      "0       1  EPIC 201111557.01          K2   CANDIDATE             1.120000   \n",
      "1       2  EPIC 201111557.01          K2   CANDIDATE             1.312588   \n",
      "2       3  EPIC 201126503.01          K2   CANDIDATE             4.190000   \n",
      "3       4  EPIC 201127519.01          K2   CANDIDATE             9.912579   \n",
      "4       5  EPIC 201127519.01          K2   CANDIDATE             8.840000   \n",
      "5       6  EPIC 201147085.01          K2   CANDIDATE             0.860000   \n",
      "6       7  EPIC 201152065.01          K2   CANDIDATE             1.450000   \n",
      "7       8  EPIC 201160662.01          K2   CANDIDATE            45.000000   \n",
      "8       9  EPIC 201164625.01          K2   CANDIDATE             4.340000   \n",
      "9      10  EPIC 201166680.01          K2   CANDIDATE             2.099851   \n",
      "\n",
      "   orbital_period  \n",
      "0        2.301830  \n",
      "1        2.302368  \n",
      "2        1.194749  \n",
      "3        6.178369  \n",
      "4        6.178870  \n",
      "5        1.175890  \n",
      "6       10.696600  \n",
      "7        1.537411  \n",
      "8        2.711890  \n",
      "9       18.105490  \n"
     ]
    }
   ],
   "source": [
    "# Filter dataset to keep only CANDIDATE and CONFIRMED objects\n",
    "print(\"🔍 FILTERING DATASET - KEEPING ONLY CANDIDATES AND CONFIRMED PLANETS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Show current distribution\n",
    "print(\"Current disposition distribution:\")\n",
    "current_disp = master_df['disposition'].value_counts()\n",
    "print(current_disp)\n",
    "print(f\"Total objects: {len(master_df):,}\")\n",
    "\n",
    "# Filter to keep only CANDIDATE and CONFIRMED\n",
    "filtered_dispositions = ['CANDIDATE', 'CONFIRMED']\n",
    "master_df_filtered = master_df[master_df['disposition'].isin(filtered_dispositions)].copy()\n",
    "\n",
    "print(f\"\\n✂️ FILTERING RESULTS:\")\n",
    "print(\"-\" * 25)\n",
    "print(f\"Objects before filtering: {len(master_df):,}\")\n",
    "print(f\"Objects after filtering: {len(master_df_filtered):,}\")\n",
    "print(f\"Objects removed (FALSE POSITIVE): {len(master_df) - len(master_df_filtered):,}\")\n",
    "print(f\"Retention rate: {(len(master_df_filtered) / len(master_df) * 100):.1f}%\")\n",
    "\n",
    "# Show new distribution\n",
    "print(f\"\\n📊 NEW DISPOSITION DISTRIBUTION:\")\n",
    "print(\"-\" * 35)\n",
    "new_disp = master_df_filtered['disposition'].value_counts()\n",
    "print(new_disp)\n",
    "\n",
    "# Show breakdown by data source\n",
    "print(f\"\\nDistribution by data source:\")\n",
    "filtered_crosstab = pd.crosstab(master_df_filtered['data_source'], master_df_filtered['disposition'], margins=True)\n",
    "print(filtered_crosstab)\n",
    "\n",
    "print(f\"\\nPercentages by source:\")\n",
    "filtered_pct = pd.crosstab(master_df_filtered['data_source'], master_df_filtered['disposition'], normalize='index') * 100\n",
    "print(filtered_pct.round(1))\n",
    "\n",
    "# Update the master dataframe\n",
    "master_df = master_df_filtered.copy()\n",
    "\n",
    "# Reset row IDs after filtering\n",
    "master_df['row_id'] = range(1, len(master_df) + 1)\n",
    "\n",
    "print(f\"\\n✅ DATASET SUCCESSFULLY FILTERED!\")\n",
    "print(f\"Final dataset contains {len(master_df):,} potential planets (candidates + confirmed)\")\n",
    "print(f\"Shape: {master_df.shape}\")\n",
    "\n",
    "# Show sample of filtered data\n",
    "print(f\"\\n📋 SAMPLE OF FILTERED DATA:\")\n",
    "sample_filtered = master_df[['row_id', 'combined_id', 'data_source', 'disposition', 'planet_radius_earth', 'orbital_period']].head(10)\n",
    "print(sample_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a1901c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 TYPE CONVERSION AND COLUMN CLEANING\n",
      "=============================================\n",
      "Starting dataset shape: (8414, 55)\n",
      "Starting columns: 55\n",
      "\n",
      "Current data types:\n",
      "float64    43\n",
      "object     11\n",
      "int64       1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "📊 MISSING VALUES ANALYSIS:\n",
      "-----------------------------------\n",
      "Columns with >90% missing values (4 columns):\n",
      "  k2_name: 100.0% missing\n",
      "  planet_mass_earth: 100.0% missing\n",
      "  planet_mass_jupiter: 100.0% missing\n",
      "  stellar_age: 100.0% missing\n",
      "\n",
      "Columns with 50-90% missing values (12 columns):\n",
      "  planet_radius_jupiter: 63.5% missing\n",
      "  distance: 57.4% missing\n",
      "  gaia_magnitude: 56.7% missing\n",
      "  v_magnitude: 56.4% missing\n",
      "  discovery_facility: 56.2% missing\n",
      "  host_star_name: 56.2% missing\n",
      "  discovery_year: 56.2% missing\n",
      "  num_planets: 56.2% missing\n",
      "  discovery_method: 56.2% missing\n",
      "  num_stars: 56.2% missing\n",
      "\n",
      "🔍 CHECKING FOR ZERO-ONLY COLUMNS:\n",
      "-----------------------------------\n",
      "Columns with only zeros: 0\n",
      "\n",
      "🔍 CHECKING FOR ZERO+NA ONLY COLUMNS:\n",
      "----------------------------------------\n",
      "Columns with only zeros and NAs: 0\n"
     ]
    }
   ],
   "source": [
    "# TYPE CONVERSION AND COLUMN CLEANING\n",
    "print(\"🔄 TYPE CONVERSION AND COLUMN CLEANING\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# First, let's analyze the current state\n",
    "print(f\"Starting dataset shape: {master_df.shape}\")\n",
    "print(f\"Starting columns: {len(master_df.columns)}\")\n",
    "\n",
    "# Check data types\n",
    "print(f\"\\nCurrent data types:\")\n",
    "current_dtypes = master_df.dtypes.value_counts()\n",
    "print(current_dtypes)\n",
    "\n",
    "# Analyze missing values per column\n",
    "print(f\"\\n📊 MISSING VALUES ANALYSIS:\")\n",
    "print(\"-\" * 35)\n",
    "missing_analysis = pd.DataFrame({\n",
    "    'Column': master_df.columns,\n",
    "    'Missing_Count': master_df.isnull().sum(),\n",
    "    'Missing_Percentage': (master_df.isnull().sum() / len(master_df)) * 100,\n",
    "    'Data_Type': master_df.dtypes,\n",
    "    'Non_Null_Count': master_df.count()\n",
    "})\n",
    "missing_analysis = missing_analysis.sort_values('Missing_Percentage', ascending=False)\n",
    "\n",
    "# Show columns with high missing percentages\n",
    "high_missing = missing_analysis[missing_analysis['Missing_Percentage'] > 90]\n",
    "print(f\"Columns with >90% missing values ({len(high_missing)} columns):\")\n",
    "for _, row in high_missing.iterrows():\n",
    "    print(f\"  {row['Column']}: {row['Missing_Percentage']:.1f}% missing\")\n",
    "\n",
    "medium_missing = missing_analysis[(missing_analysis['Missing_Percentage'] > 50) & (missing_analysis['Missing_Percentage'] <= 90)]\n",
    "print(f\"\\nColumns with 50-90% missing values ({len(medium_missing)} columns):\")\n",
    "for _, row in medium_missing.head(10).iterrows():\n",
    "    print(f\"  {row['Column']}: {row['Missing_Percentage']:.1f}% missing\")\n",
    "\n",
    "# Check for zero-only columns (excluding ID columns)\n",
    "print(f\"\\n🔍 CHECKING FOR ZERO-ONLY COLUMNS:\")\n",
    "print(\"-\" * 35)\n",
    "numeric_cols = master_df.select_dtypes(include=[np.number]).columns\n",
    "zero_only_cols = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col not in ['row_id']:  # Exclude ID columns\n",
    "        non_null_values = master_df[col].dropna()\n",
    "        if len(non_null_values) > 0 and (non_null_values == 0).all():\n",
    "            zero_only_cols.append(col)\n",
    "\n",
    "print(f\"Columns with only zeros: {len(zero_only_cols)}\")\n",
    "for col in zero_only_cols:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Check for columns with zeros + NAs only\n",
    "print(f\"\\n🔍 CHECKING FOR ZERO+NA ONLY COLUMNS:\")\n",
    "print(\"-\" * 40)\n",
    "zero_na_only_cols = []\n",
    "\n",
    "for col in numeric_cols:\n",
    "    if col not in ['row_id']:\n",
    "        non_null_values = master_df[col].dropna()\n",
    "        if len(non_null_values) > 0:\n",
    "            unique_values = set(non_null_values.unique())\n",
    "            if unique_values == {0} or unique_values == {0.0} or len(unique_values) == 0:\n",
    "                zero_na_only_cols.append(col)\n",
    "\n",
    "print(f\"Columns with only zeros and NAs: {len(zero_na_only_cols)}\")\n",
    "for col in zero_na_only_cols:\n",
    "    non_null_count = master_df[col].count()\n",
    "    print(f\"  - {col}: {non_null_count} non-null values, all zeros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb23a990",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧹 CLEANING AND TYPE CONVERSION\n",
      "===================================\n",
      "Dropping 4 columns with 100% missing values:\n",
      "  ✗ k2_name\n",
      "  ✗ planet_mass_earth\n",
      "  ✗ planet_mass_jupiter\n",
      "  ✗ stellar_age\n",
      "\n",
      "🔄 PERFORMING TYPE CONVERSIONS:\n",
      "--------------------------------\n",
      "  ✓ data_source → category\n",
      "  ✓ mission → category\n",
      "  ✓ disposition → category\n",
      "  ✓ disposition_reference → category\n",
      "  ✓ combined_id → string\n",
      "  ✓ planet_name → string\n",
      "  ✓ host_star_name → string\n",
      "  ✓ kepid → string\n",
      "  ✓ koi_name → string\n",
      "  ✓ discovery_method → string\n",
      "  ✓ discovery_facility → string\n",
      "  ✓ orbital_period → numeric\n",
      "  ✓ orbital_period_err1 → numeric\n",
      "  ✓ orbital_period_err2 → numeric\n",
      "  ✓ semi_major_axis → numeric\n",
      "  ✓ eccentricity → numeric\n",
      "  ✓ inclination → numeric\n",
      "  ✓ planet_radius_earth → numeric\n",
      "  ✓ planet_radius_earth_err1 → numeric\n",
      "  ✓ planet_radius_earth_err2 → numeric\n",
      "  ✓ equilibrium_temperature → numeric\n",
      "  ✓ insolation_flux → numeric\n",
      "  ✓ transit_epoch → numeric\n",
      "  ✓ transit_duration → numeric\n",
      "  ✓ transit_depth → numeric\n",
      "  ✓ impact_parameter → numeric\n",
      "  ✓ stellar_effective_temp → numeric\n",
      "  ✓ stellar_radius → numeric\n",
      "  ✓ stellar_mass → numeric\n",
      "  ✓ stellar_metallicity → numeric\n",
      "  ✓ stellar_surface_gravity → numeric\n",
      "  ✓ kepler_magnitude → numeric\n",
      "  ✓ v_magnitude → numeric\n",
      "  ✓ j_magnitude → numeric\n",
      "  ✓ h_magnitude → numeric\n",
      "  ✓ k_magnitude → numeric\n",
      "  ✓ gaia_magnitude → numeric\n",
      "  ✓ ra → numeric\n",
      "  ✓ dec → numeric\n",
      "  ✓ distance → numeric\n",
      "  ✓ max_single_event_stat → numeric\n",
      "  ✓ max_multiple_event_stat → numeric\n",
      "  ✓ signal_to_noise → numeric\n",
      "  ✓ num_transits → numeric\n",
      "  ✓ row_id → Int64\n",
      "  ✓ num_stars → Int64\n",
      "  ✓ num_planets → Int64\n",
      "  ✓ discovery_year → Int64\n",
      "\n",
      "📊 FINAL CLEANING SUMMARY:\n",
      "------------------------------\n",
      "Original shape: (8414, 55)\n",
      "Cleaned shape: (8414, 51)\n",
      "Columns dropped: 4\n",
      "Rows retained: 8,414\n",
      "\n",
      "Final data type distribution:\n",
      "float64           35\n",
      "string[python]     7\n",
      "Int64              4\n",
      "category           2\n",
      "category           1\n",
      "object             1\n",
      "category           1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "✅ TYPE CONVERSION AND CLEANING COMPLETE!\n",
      "Final dataset: 8,414 rows × 51 columns\n"
     ]
    }
   ],
   "source": [
    "# PERFORM TYPE CONVERSIONS AND DROP PROBLEMATIC COLUMNS\n",
    "print(f\"\\n🧹 CLEANING AND TYPE CONVERSION\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Create a copy for cleaning\n",
    "master_df_clean = master_df.copy()\n",
    "\n",
    "# 1. DROP COLUMNS WITH 100% MISSING VALUES\n",
    "columns_to_drop_100_missing = ['k2_name', 'planet_mass_earth', 'planet_mass_jupiter', 'stellar_age']\n",
    "print(f\"Dropping {len(columns_to_drop_100_missing)} columns with 100% missing values:\")\n",
    "for col in columns_to_drop_100_missing:\n",
    "    if col in master_df_clean.columns:\n",
    "        print(f\"  ✗ {col}\")\n",
    "        master_df_clean = master_df_clean.drop(columns=[col])\n",
    "\n",
    "# 2. IDENTIFY COLUMNS TO DROP (>90% missing)\n",
    "high_missing_threshold = 90\n",
    "columns_to_drop_high_missing = missing_analysis[missing_analysis['Missing_Percentage'] > high_missing_threshold]['Column'].tolist()\n",
    "columns_to_drop_high_missing = [col for col in columns_to_drop_high_missing if col not in columns_to_drop_100_missing]\n",
    "\n",
    "if columns_to_drop_high_missing:\n",
    "    print(f\"\\nDropping {len(columns_to_drop_high_missing)} additional columns with >{high_missing_threshold}% missing:\")\n",
    "    for col in columns_to_drop_high_missing:\n",
    "        if col in master_df_clean.columns:\n",
    "            print(f\"  ✗ {col}\")\n",
    "            master_df_clean = master_df_clean.drop(columns=[col])\n",
    "\n",
    "# 3. PROPER TYPE CONVERSIONS\n",
    "print(f\"\\n🔄 PERFORMING TYPE CONVERSIONS:\")\n",
    "print(\"-\" * 32)\n",
    "\n",
    "# Define categorical columns\n",
    "categorical_columns = ['data_source', 'mission', 'disposition', 'disposition_reference']\n",
    "\n",
    "# Define string/object columns that should remain as strings\n",
    "string_columns = ['combined_id', 'planet_name', 'host_star_name', 'kepid', 'koi_name', \n",
    "                 'discovery_method', 'discovery_facility']\n",
    "\n",
    "# Convert categorical columns\n",
    "for col in categorical_columns:\n",
    "    if col in master_df_clean.columns:\n",
    "        master_df_clean[col] = master_df_clean[col].astype('category')\n",
    "        print(f\"  ✓ {col} → category\")\n",
    "\n",
    "# Convert string columns to proper string type\n",
    "for col in string_columns:\n",
    "    if col in master_df_clean.columns and master_df_clean[col].dtype != 'string':\n",
    "        master_df_clean[col] = master_df_clean[col].astype('string')\n",
    "        print(f\"  ✓ {col} → string\")\n",
    "\n",
    "# Ensure numeric columns are proper numeric types\n",
    "numeric_columns = [\n",
    "    'orbital_period', 'orbital_period_err1', 'orbital_period_err2', 'semi_major_axis', \n",
    "    'eccentricity', 'inclination', 'planet_radius_earth', 'planet_radius_earth_err1', \n",
    "    'planet_radius_earth_err2', 'equilibrium_temperature', 'insolation_flux',\n",
    "    'transit_epoch', 'transit_duration', 'transit_depth', 'impact_parameter',\n",
    "    'stellar_effective_temp', 'stellar_radius', 'stellar_mass', 'stellar_metallicity',\n",
    "    'stellar_surface_gravity', 'kepler_magnitude', 'v_magnitude', 'j_magnitude',\n",
    "    'h_magnitude', 'k_magnitude', 'gaia_magnitude', 'ra', 'dec', 'distance',\n",
    "    'max_single_event_stat', 'max_multiple_event_stat', 'signal_to_noise', 'num_transits'\n",
    "]\n",
    "\n",
    "for col in numeric_columns:\n",
    "    if col in master_df_clean.columns:\n",
    "        try:\n",
    "            master_df_clean[col] = pd.to_numeric(master_df_clean[col], errors='coerce')\n",
    "            print(f\"  ✓ {col} → numeric\")\n",
    "        except:\n",
    "            print(f\"  ⚠ {col} → conversion failed\")\n",
    "\n",
    "# Convert integer columns\n",
    "integer_columns = ['row_id', 'num_stars', 'num_planets', 'discovery_year']\n",
    "for col in integer_columns:\n",
    "    if col in master_df_clean.columns:\n",
    "        # Convert to nullable integer\n",
    "        master_df_clean[col] = master_df_clean[col].astype('Int64')\n",
    "        print(f\"  ✓ {col} → Int64\")\n",
    "\n",
    "print(f\"\\n📊 FINAL CLEANING SUMMARY:\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"Original shape: {master_df.shape}\")\n",
    "print(f\"Cleaned shape: {master_df_clean.shape}\")\n",
    "print(f\"Columns dropped: {master_df.shape[1] - master_df_clean.shape[1]}\")\n",
    "print(f\"Rows retained: {len(master_df_clean):,}\")\n",
    "\n",
    "# Check final data types\n",
    "print(f\"\\nFinal data type distribution:\")\n",
    "final_dtypes = master_df_clean.dtypes.value_counts()\n",
    "print(final_dtypes)\n",
    "\n",
    "# Update the master dataframe\n",
    "master_df = master_df_clean.copy()\n",
    "\n",
    "print(f\"\\n✅ TYPE CONVERSION AND CLEANING COMPLETE!\")\n",
    "print(f\"Final dataset: {master_df.shape[0]:,} rows × {master_df.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38621418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 FINAL DATASET VALIDATION\n",
      "==============================\n",
      "📊 COLUMN COMPLETENESS SUMMARY:\n",
      "-----------------------------------\n",
      "Total columns: 51\n",
      "✅ 100% complete: 7 columns\n",
      "🟢 80-99% complete: 10 columns\n",
      "🟡 50-79% complete: 22 columns\n",
      "🔴 <50% complete: 12 columns\n",
      "\n",
      "📋 KEY COLUMNS (100% complete):\n",
      "  ✓ data_source (category)\n",
      "  ✓ disposition (category)\n",
      "  ✓ row_id (Int64)\n",
      "  ✓ mission (category)\n",
      "  ✓ dec (float64)\n",
      "  ✓ combined_id (string)\n",
      "  ✓ ra (float64)\n",
      "\n",
      "💾 MEMORY USAGE: 6.56 MB\n",
      "\n",
      "📈 KEY STATISTICS:\n",
      "       orbital_period  planet_radius_earth  equilibrium_temperature  \\\n",
      "count        8372.000             7694.000                 5461.000   \n",
      "mean           67.627               27.988                  837.144   \n",
      "std          1696.532             1302.941                  467.495   \n",
      "min             0.176                0.220                   25.000   \n",
      "25%             4.175                1.500                  517.000   \n",
      "50%             9.762                2.240                  763.000   \n",
      "75%            23.239                3.398                 1047.390   \n",
      "max        129995.778           109061.000                 6285.000   \n",
      "\n",
      "       stellar_effective_temp  kepler_magnitude  \n",
      "count                7239.000          4725.000  \n",
      "mean                 5393.347            14.340  \n",
      "std                   944.656             1.268  \n",
      "min                  2520.000             6.974  \n",
      "25%                  5003.000            13.597  \n",
      "50%                  5546.000            14.610  \n",
      "75%                  5922.000            15.326  \n",
      "max                 46696.000            17.475  \n",
      "\n",
      "🎉 DATASET READY FOR ANALYSIS!\n",
      "   - 8,414 exoplanet candidates and confirmed planets\n",
      "   - 51 high-quality features\n",
      "   - Proper data types assigned\n",
      "   - Low-quality columns removed\n",
      "   - Ready for machine learning and scientific analysis\n"
     ]
    }
   ],
   "source": [
    "# FINAL VALIDATION AND SUMMARY\n",
    "print(f\"\\n🎯 FINAL DATASET VALIDATION\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Show remaining columns and their completeness\n",
    "remaining_missing = pd.DataFrame({\n",
    "    'Column': master_df.columns,\n",
    "    'Data_Type': master_df.dtypes,\n",
    "    'Non_Null_Count': master_df.count(),\n",
    "    'Missing_Count': master_df.isnull().sum(),\n",
    "    'Missing_Percentage': (master_df.isnull().sum() / len(master_df)) * 100,\n",
    "    'Completeness': ((master_df.count() / len(master_df)) * 100).round(1)\n",
    "})\n",
    "\n",
    "# Sort by completeness (descending)\n",
    "remaining_missing = remaining_missing.sort_values('Completeness', ascending=False)\n",
    "\n",
    "print(f\"📊 COLUMN COMPLETENESS SUMMARY:\")\n",
    "print(\"-\" * 35)\n",
    "print(f\"Total columns: {len(master_df.columns)}\")\n",
    "\n",
    "# Show columns by completeness categories\n",
    "complete_cols = remaining_missing[remaining_missing['Completeness'] == 100.0]\n",
    "high_complete = remaining_missing[(remaining_missing['Completeness'] >= 80) & (remaining_missing['Completeness'] < 100)]\n",
    "medium_complete = remaining_missing[(remaining_missing['Completeness'] >= 50) & (remaining_missing['Completeness'] < 80)]\n",
    "low_complete = remaining_missing[remaining_missing['Completeness'] < 50]\n",
    "\n",
    "print(f\"✅ 100% complete: {len(complete_cols)} columns\")\n",
    "print(f\"🟢 80-99% complete: {len(high_complete)} columns\") \n",
    "print(f\"🟡 50-79% complete: {len(medium_complete)} columns\")\n",
    "print(f\"🔴 <50% complete: {len(low_complete)} columns\")\n",
    "\n",
    "# Show the most important/complete columns\n",
    "print(f\"\\n📋 KEY COLUMNS (100% complete):\")\n",
    "for _, row in complete_cols.head(10).iterrows():\n",
    "    print(f\"  ✓ {row['Column']} ({row['Data_Type']})\")\n",
    "\n",
    "# Memory usage\n",
    "memory_usage = master_df.memory_usage(deep=True).sum() / 1024**2  # Convert to MB\n",
    "print(f\"\\n💾 MEMORY USAGE: {memory_usage:.2f} MB\")\n",
    "\n",
    "# Quick statistics for key numeric columns\n",
    "key_numeric_cols = ['orbital_period', 'planet_radius_earth', 'equilibrium_temperature', \n",
    "                   'stellar_effective_temp', 'kepler_magnitude']\n",
    "available_key_cols = [col for col in key_numeric_cols if col in master_df.columns]\n",
    "\n",
    "if available_key_cols:\n",
    "    print(f\"\\n📈 KEY STATISTICS:\")\n",
    "    key_stats = master_df[available_key_cols].describe().round(3)\n",
    "    print(key_stats)\n",
    "\n",
    "print(f\"\\n🎉 DATASET READY FOR ANALYSIS!\")\n",
    "print(f\"   - {len(master_df):,} exoplanet candidates and confirmed planets\")\n",
    "print(f\"   - {len(master_df.columns)} high-quality features\")\n",
    "print(f\"   - Proper data types assigned\")\n",
    "print(f\"   - Low-quality columns removed\")\n",
    "print(f\"   - Ready for machine learning and scientific analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b911bccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define low importance features to drop (based on feature importance analysis)\n",
    "low_importance_features = [\n",
    "    'stellar_surface_gravity',    # 0.0094 (0.9%)\n",
    "    'j_magnitude',               # 0.0092 (0.9%)\n",
    "    'kepler_magnitude',          # 0.0092 (0.9%)\n",
    "    'h_magnitude',               # 0.0085 (0.9%)\n",
    "    'planet_radius_jupiter',     # 0.0085 (0.8%)\n",
    "    'discovery_year',            # 0.0062 (0.6%)\n",
    "    'eccentricity',              # 0.0008 (0.1%)\n",
    "    'num_stars'                  # 0.0004 (0.0%)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "46225254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST TRAINING AND EVALUATION\n",
      "=============================================\n",
      "Preparing data for machine learning...\n",
      "Selected 38 features for modeling\n",
      "After dropping 8 low importance features: 30 features remaining\n",
      "Target encoding: CANDIDATE = 0, CONFIRMED = 1\n",
      "Class distribution:\n",
      "  CANDIDATE: 3,353 (39.9%)\n",
      "  CONFIRMED: 5,061 (60.1%)\n",
      "\n",
      "Original missing values: 77412\n",
      "Remaining missing values: 0\n"
     ]
    }
   ],
   "source": [
    "# RANDOM FOREST MACHINE LEARNING MODEL\n",
    "print(\"RANDOM FOREST TRAINING AND EVALUATION\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Import required libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (accuracy_score, roc_auc_score, classification_report, \n",
    "                           confusion_matrix, precision_recall_fscore_support)\n",
    "import numpy as np\n",
    "\n",
    "# Prepare features and target\n",
    "print(\"Preparing data for machine learning...\")\n",
    "\n",
    "# Select numeric features for modeling\n",
    "numeric_features = master_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "# Remove ID columns and target-related columns\n",
    "exclude_columns = ['row_id']\n",
    "feature_columns = [col for col in numeric_features if col not in exclude_columns]\n",
    "\n",
    "# Prepare feature matrix\n",
    "X = master_df[feature_columns].copy()\n",
    "print(f\"Selected {len(feature_columns)} features for modeling\")\n",
    "\n",
    "# Drop low importance features identified from feature importance analysis\n",
    "X = X.drop(columns=low_importance_features, errors='ignore')\n",
    "print(f\"After dropping {len(low_importance_features)} low importance features: {X.shape[1]} features remaining\")\n",
    "\n",
    "# Prepare target variable (encode disposition as binary)\n",
    "# CONFIRMED = 1, CANDIDATE = 0\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(master_df['disposition'])\n",
    "target_classes = label_encoder.classes_\n",
    "\n",
    "print(f\"Target encoding: {target_classes[0]} = 0, {target_classes[1]} = 1\")\n",
    "print(f\"Class distribution:\")\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "for i, (cls, count) in enumerate(zip(unique, counts)):\n",
    "    percentage = (count / len(y)) * 100\n",
    "    print(f\"  {target_classes[i]}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nOriginal missing values: {X.isnull().sum().sum()}\")\n",
    "\n",
    "# Handle missing values using imputation\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(X), \n",
    "    columns=X.columns, \n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(f\"Remaining missing values: {X_imputed.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcaf5496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training set: 6,731 samples\n",
      "Test set: 1,683 samples\n",
      "Features: 30\n",
      "\n",
      "Class distribution in training set:\n",
      "  CANDIDATE: 2,682 (39.8%)\n",
      "  CONFIRMED: 4,049 (60.2%)\n",
      "\n",
      "Class distribution in test set:\n",
      "  CANDIDATE: 671 (39.9%)\n",
      "  CONFIRMED: 1,012 (60.1%)\n",
      "\n",
      "Training Random Forest model...\n",
      "Random Forest model trained successfully\n",
      "Training accuracy: 1.0000\n",
      "Test accuracy: 0.9299\n",
      "Test AUC-ROC: 0.9739\n",
      "Cross-validation accuracy: 0.9326 (±0.0147)\n"
     ]
    }
   ],
   "source": [
    "# Train-test split and model training\n",
    "test_size = 0.2\n",
    "random_state = 42\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_imputed, y, \n",
    "    test_size=test_size, \n",
    "    random_state=random_state, \n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining set: {X_train.shape[0]:,} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]:,} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "print(f\"\\nClass distribution in training set:\")\n",
    "train_classes, train_counts = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(train_classes, train_counts):\n",
    "    class_name = target_classes[cls]\n",
    "    percentage = (count / len(y_train)) * 100\n",
    "    print(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nClass distribution in test set:\")\n",
    "test_classes, test_counts = np.unique(y_test, return_counts=True)\n",
    "for cls, count in zip(test_classes, test_counts):\n",
    "    class_name = target_classes[cls]\n",
    "    percentage = (count / len(y_test)) * 100\n",
    "    print(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Create and train Random Forest model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    random_state=random_state,\n",
    "    n_jobs=-1  # Use all available cores\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining Random Forest model...\")\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf_model.predict(X_train)\n",
    "y_test_pred = rf_model.predict(X_test)\n",
    "y_test_proba = rf_model.predict_proba(X_test)[:, 1]  # Probability for CONFIRMED class\n",
    "\n",
    "# Calculate basic metrics\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "\n",
    "print(\"Random Forest model trained successfully\")\n",
    "print(f\"Training accuracy: {train_accuracy:.4f}\")\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
    "\n",
    "# Cross-validation\n",
    "cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "print(f\"Cross-validation accuracy: {cv_scores.mean():.4f} (±{cv_scores.std()*2:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61724b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DETAILED PERFORMANCE ANALYSIS\n",
      "========================================\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CANDIDATE     0.9382    0.8823    0.9094       671\n",
      "   CONFIRMED     0.9249    0.9615    0.9428      1012\n",
      "\n",
      "    accuracy                         0.9299      1683\n",
      "   macro avg     0.9315    0.9219    0.9261      1683\n",
      "weighted avg     0.9302    0.9299    0.9295      1683\n",
      "\n",
      "\n",
      "Detailed Metrics by Class:\n",
      "CANDIDATE:\n",
      "  Precision: 0.9382\n",
      "  Recall: 0.8823\n",
      "  F1-Score: 0.9094\n",
      "  Support: 671\n",
      "CONFIRMED:\n",
      "  Precision: 0.9249\n",
      "  Recall: 0.9615\n",
      "  F1-Score: 0.9428\n",
      "  Support: 1012\n",
      "\n",
      "Macro Average:\n",
      "  Precision: 0.9315\n",
      "  Recall: 0.9219\n",
      "  F1-Score: 0.9261\n",
      "\n",
      "Micro Average:\n",
      "  Precision: 0.9299\n",
      "  Recall: 0.9299\n",
      "  F1-Score: 0.9299\n"
     ]
    }
   ],
   "source": [
    "# DETAILED PERFORMANCE METRICS\n",
    "print(\"\\nDETAILED PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Classification Report with Precision and Recall\n",
    "print(\"\\nClassification Report:\")\n",
    "class_names = ['CANDIDATE', 'CONFIRMED']\n",
    "report = classification_report(y_test, y_test_pred, target_names=class_names, digits=4)\n",
    "print(report)\n",
    "\n",
    "# Precision, Recall, F1-score by class\n",
    "precision, recall, f1, support = precision_recall_fscore_support(y_test, y_test_pred, average=None)\n",
    "print(\"\\nDetailed Metrics by Class:\")\n",
    "for i, class_name in enumerate(class_names):\n",
    "    print(f\"{class_name}:\")\n",
    "    print(f\"  Precision: {precision[i]:.4f}\")\n",
    "    print(f\"  Recall: {recall[i]:.4f}\")\n",
    "    print(f\"  F1-Score: {f1[i]:.4f}\")\n",
    "    print(f\"  Support: {support[i]}\")\n",
    "\n",
    "# Macro and Micro averages\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='macro')\n",
    "micro_precision, micro_recall, micro_f1, _ = precision_recall_fscore_support(y_test, y_test_pred, average='micro')\n",
    "\n",
    "print(f\"\\nMacro Average:\")\n",
    "print(f\"  Precision: {macro_precision:.4f}\")\n",
    "print(f\"  Recall: {macro_recall:.4f}\")\n",
    "print(f\"  F1-Score: {macro_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nMicro Average:\")\n",
    "print(f\"  Precision: {micro_precision:.4f}\")\n",
    "print(f\"  Recall: {micro_recall:.4f}\")\n",
    "print(f\"  F1-Score: {micro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f8912f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CONFUSION MATRIX\n",
      "====================\n",
      "\n",
      "Confusion Matrix:\n",
      "                Predicted\n",
      "                CANDIDATE  CONFIRMED\n",
      "Actual CANDIDATE      592        79\n",
      "       CONFIRMED       39       973\n",
      "\n",
      "Confusion Matrix (Percentages):\n",
      "                Predicted\n",
      "                CANDIDATE  CONFIRMED\n",
      "Actual CANDIDATE     88.2%      11.8%\n",
      "       CONFIRMED      3.9%      96.1%\n",
      "\n",
      "Confusion Matrix Components:\n",
      "True Negatives (TN): 592\n",
      "False Positives (FP): 79\n",
      "False Negatives (FN): 39\n",
      "True Positives (TP): 973\n",
      "\n",
      "Additional Metrics:\n",
      "Sensitivity (Recall): 0.9615\n",
      "Specificity: 0.8823\n",
      "False Positive Rate: 0.1177\n",
      "False Negative Rate: 0.0385\n"
     ]
    }
   ],
   "source": [
    "# CONFUSION MATRIX ANALYSIS\n",
    "print(\"\\nCONFUSION MATRIX\")\n",
    "print(\"=\" * 20)\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                CANDIDATE  CONFIRMED\")\n",
    "print(f\"Actual CANDIDATE     {cm[0,0]:4d}      {cm[0,1]:4d}\")\n",
    "print(f\"       CONFIRMED     {cm[1,0]:4d}      {cm[1,1]:4d}\")\n",
    "\n",
    "# Calculate confusion matrix percentages\n",
    "cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100\n",
    "print(\"\\nConfusion Matrix (Percentages):\")\n",
    "print(f\"                Predicted\")\n",
    "print(f\"                CANDIDATE  CONFIRMED\")\n",
    "print(f\"Actual CANDIDATE    {cm_percent[0,0]:5.1f}%     {cm_percent[0,1]:5.1f}%\")\n",
    "print(f\"       CONFIRMED    {cm_percent[1,0]:5.1f}%     {cm_percent[1,1]:5.1f}%\")\n",
    "\n",
    "# True/False Positives and Negatives\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nConfusion Matrix Components:\")\n",
    "print(f\"True Negatives (TN): {tn}\")\n",
    "print(f\"False Positives (FP): {fp}\")\n",
    "print(f\"False Negatives (FN): {fn}\")\n",
    "print(f\"True Positives (TP): {tp}\")\n",
    "\n",
    "# Additional metrics from confusion matrix\n",
    "specificity = tn / (tn + fp)\n",
    "sensitivity = tp / (tp + fn)  # Same as recall\n",
    "print(f\"\\nAdditional Metrics:\")\n",
    "print(f\"Sensitivity (Recall): {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "print(f\"False Positive Rate: {fp/(fp+tn):.4f}\")\n",
    "print(f\"False Negative Rate: {fn/(fn+tp):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d883d07d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "FEATURE IMPORTANCE ANALYSIS\n",
      "===================================\n",
      "\n",
      "Top 20 Most Important Features:\n",
      "---------------------------------------------\n",
      " 1. num_planets                    0.2601\n",
      " 2. max_multiple_event_stat        0.1157\n",
      " 3. signal_to_noise                0.1145\n",
      " 4. orbital_period                 0.0308\n",
      " 5. disposition_score              0.0301\n",
      " 6. planet_radius_earth            0.0299\n",
      " 7. planet_radius_earth_err1       0.0290\n",
      " 8. dec                            0.0273\n",
      " 9. orbital_period_err1            0.0248\n",
      "10. semi_major_axis                0.0236\n",
      "11. planet_radius_earth_err2       0.0235\n",
      "12. orbital_period_err2            0.0226\n",
      "13. transit_duration               0.0195\n",
      "14. max_single_event_stat          0.0194\n",
      "15. ra                             0.0192\n",
      "16. stellar_metallicity            0.0182\n",
      "17. transit_depth                  0.0160\n",
      "18. stellar_radius                 0.0156\n",
      "19. k_magnitude                    0.0151\n",
      "20. num_transits                   0.0149\n",
      "\n",
      "Feature Importance Statistics:\n",
      "Total features: 30\n",
      "Mean importance: 0.0333\n",
      "Std importance: 0.0497\n",
      "Max importance: 0.2601\n",
      "Min importance: 0.0107\n",
      "\n",
      "Features above average importance: 3\n",
      "Features contributing to 80% of importance: 16\n",
      "\n",
      "Top 10 Most Important Features for Prediction:\n",
      "--------------------------------------------------\n",
      " 1. num_planets                    0.2601 (26.0%)\n",
      " 2. max_multiple_event_stat        0.1157 (11.6%)\n",
      " 3. signal_to_noise                0.1145 (11.5%)\n",
      " 4. orbital_period                 0.0308 (3.1%)\n",
      " 5. disposition_score              0.0301 (3.0%)\n",
      " 6. planet_radius_earth            0.0299 (3.0%)\n",
      " 7. planet_radius_earth_err1       0.0290 (2.9%)\n",
      " 8. dec                            0.0273 (2.7%)\n",
      " 9. orbital_period_err1            0.0248 (2.5%)\n",
      "10. semi_major_axis                0.0236 (2.4%)\n",
      "11. planet_radius_earth_err2       0.0235 (2.4%)\n",
      "12. orbital_period_err2            0.0226 (2.3%)\n",
      "13. transit_duration               0.0195 (2.0%)\n",
      "14. max_single_event_stat          0.0194 (1.9%)\n",
      "15. ra                             0.0192 (1.9%)\n",
      "16. stellar_metallicity            0.0182 (1.8%)\n",
      "17. transit_depth                  0.0160 (1.6%)\n",
      "18. stellar_radius                 0.0156 (1.6%)\n",
      "19. k_magnitude                    0.0151 (1.5%)\n",
      "20. num_transits                   0.0149 (1.5%)\n",
      "21. v_magnitude                    0.0146 (1.5%)\n",
      "22. impact_parameter               0.0145 (1.4%)\n",
      "23. distance                       0.0144 (1.4%)\n",
      "24. transit_epoch                  0.0142 (1.4%)\n",
      "25. stellar_mass                   0.0129 (1.3%)\n",
      "26. inclination                    0.0128 (1.3%)\n",
      "27. stellar_effective_temp         0.0128 (1.3%)\n",
      "28. equilibrium_temperature        0.0122 (1.2%)\n",
      "29. insolation_flux                0.0112 (1.1%)\n",
      "30. gaia_magnitude                 0.0107 (1.1%)\n",
      "\n",
      "Model training and evaluation completed successfully\n",
      "Final model ready for predictions and further analysis\n"
     ]
    }
   ],
   "source": [
    "# FEATURE IMPORTANCE ANALYSIS\n",
    "print(\"\\nFEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Get feature importances\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Most Important Features:\")\n",
    "print(\"-\" * 45)\n",
    "for i, (_, row) in enumerate(feature_importance.head(20).iterrows(), 1):\n",
    "    print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f}\")\n",
    "\n",
    "# Feature importance statistics\n",
    "print(f\"\\nFeature Importance Statistics:\")\n",
    "print(f\"Total features: {len(feature_importance)}\")\n",
    "print(f\"Mean importance: {feature_importance['importance'].mean():.4f}\")\n",
    "print(f\"Std importance: {feature_importance['importance'].std():.4f}\")\n",
    "print(f\"Max importance: {feature_importance['importance'].max():.4f}\")\n",
    "print(f\"Min importance: {feature_importance['importance'].min():.4f}\")\n",
    "\n",
    "# Features with importance > mean\n",
    "important_features = feature_importance[feature_importance['importance'] > feature_importance['importance'].mean()]\n",
    "print(f\"\\nFeatures above average importance: {len(important_features)}\")\n",
    "\n",
    "# Features contributing to 80% of total importance\n",
    "cumulative_importance = feature_importance['importance'].cumsum() / feature_importance['importance'].sum()\n",
    "top_80_count = (cumulative_importance <= 0.8).sum() + 1\n",
    "print(f\"Features contributing to 80% of importance: {top_80_count}\")\n",
    "\n",
    "print(\"\\nTop 10 Most Important Features for Prediction:\")\n",
    "print(\"-\" * 50)\n",
    "for i, (_, row) in enumerate(feature_importance.iterrows(), 1):\n",
    "    importance_pct = (row['importance'] / feature_importance['importance'].sum()) * 100\n",
    "    print(f\"{i:2d}. {row['feature']:<30} {row['importance']:.4f} ({importance_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nModel training and evaluation completed successfully\")\n",
    "print(\"Final model ready for predictions and further analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d407b721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f0192486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "⚙️ HYPERPARAMETER TUNING:\n",
      "----------------------------\n",
      "Parameter grid defined with 216 combinations\n",
      "🔍 Starting grid search...\n",
      "Fitting 3 folds for each of 216 candidates, totalling 648 fits\n",
      "\n",
      "✅ Grid search completed!\n",
      "Best parameters: {'max_depth': 30, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best cross-validation AUC: 0.9794\n",
      "\n",
      "📊 EVALUATING BEST MODEL:\n",
      "---------------------------\n",
      "Best model performance:\n",
      "  Training accuracy: 1.0000\n",
      "  Test accuracy: 0.9293\n",
      "  Test AUC-ROC: 0.9751\n",
      "\n",
      "📋 DETAILED CLASSIFICATION REPORT:\n",
      "-------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   CANDIDATE     0.9340    0.8852    0.9090       671\n",
      "   CONFIRMED     0.9265    0.9585    0.9422      1012\n",
      "\n",
      "    accuracy                         0.9293      1683\n",
      "   macro avg     0.9302    0.9219    0.9256      1683\n",
      "weighted avg     0.9294    0.9293    0.9289      1683\n",
      "\n",
      "\n",
      "🔍 CONFUSION MATRIX:\n",
      "-------------------\n",
      "Actual\\Predicted  CANDIDATE  CONFIRMED\n",
      "CANDIDATE               594         77\n",
      "CONFIRMED                42        970\n",
      "\n",
      "📈 ADDITIONAL METRICS:\n",
      "  Precision (CONFIRMED): 0.9265\n",
      "  Recall (CONFIRMED):    0.9585\n",
      "  Precision (CANDIDATE): 0.9340\n",
      "  Recall (CANDIDATE):    0.8852\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# HYPERPARAMETER TUNING\n",
    "print(f\"\\n⚙️ HYPERPARAMETER TUNING:\")\n",
    "print(\"-\" * 28)\n",
    "\n",
    "# Define parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2']\n",
    "}\n",
    "\n",
    "print(f\"Parameter grid defined with {np.prod([len(v) for v in param_grid.values()])} combinations\")\n",
    "\n",
    "# Perform grid search with cross-validation\n",
    "rf_grid = RandomForestClassifier(random_state=random_state, n_jobs=-1)\n",
    "grid_search = GridSearchCV(\n",
    "    rf_grid, \n",
    "    param_grid, \n",
    "    cv=3,  # Use 3-fold CV for speed\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"🔍 Starting grid search...\")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get best model\n",
    "best_rf = grid_search.best_estimator_\n",
    "print(f\"\\n✅ Grid search completed!\")\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation AUC: {grid_search.best_score_:.4f}\")\n",
    "\n",
    "# EVALUATE BEST MODEL\n",
    "print(f\"\\n📊 EVALUATING BEST MODEL:\")\n",
    "print(\"-\" * 27)\n",
    "\n",
    "# Make predictions with best model\n",
    "y_train_pred_best = best_rf.predict(X_train)\n",
    "y_test_pred_best = best_rf.predict(X_test)\n",
    "y_test_proba_best = best_rf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "train_accuracy_best = accuracy_score(y_train, y_train_pred_best)\n",
    "test_accuracy_best = accuracy_score(y_test, y_test_pred_best)\n",
    "test_auc_best = roc_auc_score(y_test, y_test_proba_best)\n",
    "\n",
    "print(f\"Best model performance:\")\n",
    "print(f\"  Training accuracy: {train_accuracy_best:.4f}\")\n",
    "print(f\"  Test accuracy: {test_accuracy_best:.4f}\")\n",
    "print(f\"  Test AUC-ROC: {test_auc_best:.4f}\")\n",
    "\n",
    "# Detailed classification report\n",
    "print(f\"\\n📋 DETAILED CLASSIFICATION REPORT:\")\n",
    "print(\"-\" * 37)\n",
    "class_report = classification_report(\n",
    "    y_test, y_test_pred_best, \n",
    "    target_names=target_classes,\n",
    "    digits=4\n",
    ")\n",
    "print(class_report)\n",
    "\n",
    "# Confusion Matrix\n",
    "print(f\"\\n🔍 CONFUSION MATRIX:\")\n",
    "print(\"-\" * 19)\n",
    "cm = confusion_matrix(y_test, y_test_pred_best)\n",
    "print(\"Actual\\\\Predicted  CANDIDATE  CONFIRMED\")\n",
    "print(f\"CANDIDATE         {cm[0,0]:9d}  {cm[0,1]:9d}\")\n",
    "print(f\"CONFIRMED         {cm[1,0]:9d}  {cm[1,1]:9d}\")\n",
    "\n",
    "# Calculate additional metrics\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "precision_confirmed = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "recall_confirmed = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "precision_candidate = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "recall_candidate = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "print(f\"\\n📈 ADDITIONAL METRICS:\")\n",
    "print(f\"  Precision (CONFIRMED): {precision_confirmed:.4f}\")\n",
    "print(f\"  Recall (CONFIRMED):    {recall_confirmed:.4f}\")\n",
    "print(f\"  Precision (CANDIDATE): {precision_candidate:.4f}\")\n",
    "print(f\"  Recall (CANDIDATE):    {recall_candidate:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92992f42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e414df19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAVING RANDOM FOREST MODEL\n",
      "==============================\n",
      "✅ Model saved successfully as: models\\merged_dataset_rf_model.joblib\n",
      "📊 Model type: RandomForestClassifier\n",
      "🎯 Model parameters: {'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 30, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 300, 'n_jobs': -1, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "✅ Model components saved as: models\\merged_dataset_rf_components.joblib\n",
      "📋 Components included: ['model', 'imputer', 'label_encoder', 'feature_columns', 'target_classes', 'low_importance_features']\n",
      "\n",
      "📁 File sizes:\n",
      "   Model only: 18505.3 KB\n",
      "   Full components: 18507.8 KB\n",
      "\n",
      "💡 To load the model later:\n",
      "   model = joblib.load('models\\merged_dataset_rf_model.joblib')\n",
      "   components = joblib.load('models\\merged_dataset_rf_components.joblib')\n",
      "✅ Model components saved as: models\\merged_dataset_rf_components.joblib\n",
      "📋 Components included: ['model', 'imputer', 'label_encoder', 'feature_columns', 'target_classes', 'low_importance_features']\n",
      "\n",
      "📁 File sizes:\n",
      "   Model only: 18505.3 KB\n",
      "   Full components: 18507.8 KB\n",
      "\n",
      "💡 To load the model later:\n",
      "   model = joblib.load('models\\merged_dataset_rf_model.joblib')\n",
      "   components = joblib.load('models\\merged_dataset_rf_components.joblib')\n"
     ]
    }
   ],
   "source": [
    "# SAVE TRAINED MODEL AS JOBLIB FILE\n",
    "print(\"SAVING RANDOM FOREST MODEL\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Create models directory if it doesn't exist\n",
    "models_dir = \"models\"\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "    print(f\"Created directory: {models_dir}\")\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "model_filename = \"merged_dataset_rf_model.joblib\"\n",
    "model_path = os.path.join(models_dir, model_filename)\n",
    "\n",
    "# Save the best Random Forest model\n",
    "joblib.dump(best_rf, model_path)\n",
    "\n",
    "print(f\"✅ Model saved successfully as: {model_path}\")\n",
    "print(f\"📊 Model type: {type(best_rf).__name__}\")\n",
    "print(f\"🎯 Model parameters: {best_rf.get_params()}\")\n",
    "\n",
    "# Also save additional components that might be needed for predictions\n",
    "components_filename = \"merged_dataset_rf_components.joblib\"\n",
    "components_path = os.path.join(models_dir, components_filename)\n",
    "\n",
    "# Save imputer, label encoder, and feature columns for future predictions\n",
    "model_components = {\n",
    "    'model': best_rf,\n",
    "    'imputer': imputer,\n",
    "    'label_encoder': label_encoder,\n",
    "    'feature_columns': X.columns.tolist(),\n",
    "    'target_classes': target_classes,\n",
    "    'low_importance_features': low_importance_features\n",
    "}\n",
    "\n",
    "joblib.dump(model_components, components_path)\n",
    "\n",
    "print(f\"✅ Model components saved as: {components_path}\")\n",
    "print(f\"📋 Components included: {list(model_components.keys())}\")\n",
    "\n",
    "# Display file sizes\n",
    "model_size = os.path.getsize(model_path) / 1024  # KB\n",
    "components_size = os.path.getsize(components_path) / 1024  # KB\n",
    "\n",
    "print(f\"\\n📁 File sizes:\")\n",
    "print(f\"   Model only: {model_size:.1f} KB\")\n",
    "print(f\"   Full components: {components_size:.1f} KB\")\n",
    "\n",
    "print(f\"\\n💡 To load the model later:\")\n",
    "print(f\"   model = joblib.load('{model_path}')\")\n",
    "print(f\"   components = joblib.load('{components_path}')\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
